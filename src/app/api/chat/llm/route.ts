import { NextRequest, NextResponse } from 'next/server';
import { createClient } from '@supabase/supabase-js';
import { llmManager } from '@/services/llm';
import { DeepSeekProvider } from '@/services/llm/providers';

import type { AppContext, ChatMessage } from '@/services/llm/types';
import { simpleLogger as logger } from '@/utils/logger';

// Fonction pour cr√©er le client Supabase
const createSupabaseClient = () => {
  const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
  const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

  if (!supabaseUrl || !supabaseServiceKey) {
    throw new Error('Variables d\'environnement Supabase manquantes');
  }

  return createClient(supabaseUrl, supabaseServiceKey);
};

// Fonction pour r√©cup√©rer un agent
const getAgentById = async (id: string) => {
  try {
    const supabase = createSupabaseClient();
    const { data, error } = await supabase
      .from('agents')
      .select('*')
      .eq('id', id)
      .single();

    if (error) {
      logger.error('Erreur lors de la r√©cup√©ration de l\'agent:', error);
      return null;
    }

    return data;
  } catch (error) {
    logger.error('Erreur getAgentById:', error);
    return null;
  }
};

export async function POST(request: NextRequest) {
  logger.dev("[LLM API] üöÄ REQU√äTE RE√áUE !");
  try {
    // V√©rifier l'authentification
    const authHeader = request.headers.get('authorization');
    let userId: string;

    if (authHeader && authHeader.startsWith('Bearer ')) {
      const userToken = authHeader.substring(7);
      const supabase = createSupabaseClient();
      const { data: { user }, error: authError } = await supabase.auth.getUser(userToken);
      
      if (authError || !user) {
        logger.dev("[LLM API] ‚ùå Token invalide ou expir√©");
        return NextResponse.json(
          { error: 'Token invalide ou expir√©' },
          { status: 401 }
        );
      }
      userId = user.id;
      logger.dev("[LLM API] ‚úÖ Utilisateur authentifi√©:", userId);
    } else {
      logger.dev("[LLM API] ‚ùå Token d'authentification manquant");
      return NextResponse.json(
        { error: 'Authentification requise' },
        { status: 401 }
      );
    }

    const { message, context, history, provider, channelId: incomingChannelId } = await request.json();
    
    logger.dev("[LLM API] üöÄ D√©but de la requ√™te");
    logger.dev("[LLM API] üë§ Utilisateur:", userId);
    logger.dev("[LLM API] üì¶ Body re√ßu:", { message, context, provider });

                    // R√©cup√©rer la configuration de l'agent si sp√©cifi√©e
                let agentConfig = null;
                if (context?.agentId) {
                  logger.dev("[LLM API] üîç Recherche agent avec ID:", context.agentId);
                  agentConfig = await getAgentById(context.agentId);
                  logger.dev("[LLM API] ü§ñ Configuration agent trouv√©e:", agentConfig?.name);
                  if (agentConfig) {
                                      if (agentConfig.system_instructions) {
                    logger.dev("[LLM API] üìù Instructions syst√®me (extrait):", agentConfig.system_instructions.substring(0, 200) + '...');
                  }
                  } else {
                    logger.dev("[LLM API] ‚ö†Ô∏è Agent non trouv√© pour l'ID:", context.agentId);
                  }
                } else {
                  logger.dev("[LLM API] ‚ö†Ô∏è Aucun agentId fourni dans le contexte");
                }

    // D√©terminer le provider √† utiliser
    let targetProvider = provider;
    
    // PRIORIT√â 1: Agent s√©lectionn√© (priorit√© absolue)
    if (agentConfig && agentConfig.provider) {
      targetProvider = agentConfig.provider;
      logger.dev("[LLM API] üéØ Agent s√©lectionn√© - Forcer provider:", agentConfig.provider, "pour l'agent:", agentConfig.name);
    }
    // PRIORIT√â 2: Provider manuel (menu kebab)
    else if (provider) {
      targetProvider = provider;
      logger.dev("[LLM API] üîß Provider manuel s√©lectionn√©:", provider);
    }
    // PRIORIT√â 3: Provider par d√©faut
    else {
      targetProvider = 'synesia';
      logger.dev("[LLM API] ‚öôÔ∏è Utilisation du provider par d√©faut:", targetProvider);
    }

    // Changer de provider si n√©cessaire
    if (targetProvider && targetProvider !== llmManager.getCurrentProviderId()) {
      llmManager.setProvider(targetProvider);
      logger.dev("[LLM API] üîÑ Provider chang√© vers:", targetProvider);
    }

    // Pr√©parer le contexte par d√©faut si non fourni
    const appContext: AppContext = context || {
      type: 'chat_session',
      id: 'default',
      name: 'Chat g√©n√©ral'
    };

    // Utiliser le provider manager
    const currentProvider = llmManager.getCurrentProvider();
    if (!currentProvider) {
      throw new Error('Aucun provider LLM disponible');
    }

    logger.dev("[LLM API] üöÄ Provider utilis√©:", currentProvider.id);

    // V√©rifier si c'est DeepSeek pour le streaming
    if (currentProvider.id === 'deepseek') {
      logger.dev("[LLM API] üöÄ Streaming avec DeepSeek");
      
      // Cr√©er un canal unique pour le streaming
      const channelId = incomingChannelId || `llm-stream-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      logger.dev("[LLM API] üì° Canal utilis√©:", channelId);
      
      // Utiliser le provider avec configuration d'agent
      const deepseekProvider = new DeepSeekProvider();
      logger.dev("[LLM API] üîß Configuration avant merge:", {
        defaultModel: deepseekProvider.getDefaultConfig().model,
        defaultInstructions: deepseekProvider.getDefaultConfig().system_instructions?.substring(0, 50) + '...'
      });
      
      const config = deepseekProvider['mergeConfigWithAgent'](agentConfig || undefined);
      logger.dev("[LLM API] üîß Configuration apr√®s merge:", {
        model: config.model,
        temperature: config.temperature,
        instructions: config.system_instructions?.substring(0, 100) + '...'
      });
      
      // Pr√©parer les messages avec la configuration dynamique
      const systemContent = deepseekProvider['formatContext'](appContext, config);
      logger.dev("[LLM API] üìù Contenu syst√®me pr√©par√©:", systemContent.substring(0, 200) + '...');
      
      const messages = [
        {
          role: 'system' as const,
          content: systemContent
        },
        ...history.map((msg: ChatMessage) => ({
          role: msg.role as 'user' | 'assistant' | 'system',
          content: msg.content
        })),
        {
          role: 'user' as const,
          content: message
        }
      ];

      // Appeler DeepSeek avec streaming et configuration dynamique
      const payload = {
        model: config.model,
        messages,
        stream: true,
        temperature: config.temperature,
        max_tokens: config.max_tokens,
        top_p: config.top_p
      };

      logger.dev("[LLM API] üì§ Payload complet envoy√© √† DeepSeek:");
      logger.dev(JSON.stringify(payload, null, 2));
      logger.dev("[LLM API] üì§ Appel DeepSeek avec streaming");

      const response = await fetch('https://api.deepseek.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`
        },
        body: JSON.stringify(payload)
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`DeepSeek API error: ${response.status} - ${errorText}`);
      }

      // Lire le stream et broadcaster chaque token
      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('Pas de body de r√©ponse pour le streaming');
      }

      const decoder = new TextDecoder();
      let fullResponse = '';

      logger.dev("[LLM API] üìù D√©but du streaming...");

      try {
        while (true) {
          const { done, value } = await reader.read();
          
          if (done) break;
          
          const chunk = decoder.decode(value);
          const lines = chunk.split('\n');
          
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              try {
                const data = JSON.parse(line.slice(6));
                
                if (data.choices && data.choices[0]?.delta?.content) {
                  const token = data.choices[0].delta.content;
                  fullResponse += token;
                  
                  // Broadcaster le token via Supabase Realtime
                  // Utiliser l'ID de session depuis le contexte ou un ID unique
                  const sessionId = context?.sessionId || appContext.id;
                  try {
                    const supabase = createSupabaseClient();
                    await supabase.channel(channelId).send({
                      type: 'broadcast',
                      event: 'llm-token',
                      payload: { 
                        token, 
                        sessionId,
                        fullResponse 
                      }
                    });
                    
                    logger.dev("[LLM API] üìù Token broadcast√©:", token);
                  } catch (broadcastError) {
                    logger.error("[LLM API] ‚ùå Erreur broadcast token:", broadcastError);
                  }
                } else if (data.choices && data.choices[0]?.finish_reason) {
                  logger.dev("[LLM API] ‚úÖ Streaming termin√©");
                  
                  // Broadcaster la fin du stream
                  const sessionId = context?.sessionId || appContext.id;
                  try {
                    const supabase = createSupabaseClient();
                    await supabase.channel(channelId).send({
                      type: 'broadcast',
                      event: 'llm-complete',
                      payload: { 
                        sessionId,
                        fullResponse 
                      }
                    });
                  } catch (broadcastError) {
                    logger.error("[LLM API] ‚ùå Erreur broadcast completion:", broadcastError);
                  }
                  
                  break;
                }
              } catch (e) {
                logger.warn("[LLM API] ‚ö†Ô∏è Erreur parsing SSE:", e);
              }
            }
          }
        }
      } catch (streamError) {
        logger.error("[LLM API] ‚ùå Erreur streaming:", streamError);
        
        // Essayer de broadcaster une erreur
        try {
          const sessionId = context?.sessionId || appContext.id;
          const supabase = createSupabaseClient();
          await supabase.channel(channelId).send({
            type: 'broadcast',
            event: 'llm-error',
            payload: { 
              sessionId,
              error: 'Erreur lors du streaming'
            }
          });
        } catch (broadcastError) {
          logger.error("[LLM API] ‚ùå Erreur broadcast error:", broadcastError);
        }
        
        throw streamError;
      }

      logger.dev("[LLM API] ‚úÖ Streaming termin√©, r√©ponse compl√®te:", fullResponse);

      return NextResponse.json({
        channelId,
        success: true,
        provider: llmManager.getCurrentProviderId(),
        message: "Streaming d√©marr√©, √©coutez le canal pour les tokens"
      });

    } else {
      // Fallback pour les autres providers (Synesia)
      logger.dev("[LLM API] üöÄ Appel non-streaming avec", currentProvider.name);
      
      const response = await currentProvider.call(message, appContext, history);
      
      return NextResponse.json({
        response,
        success: true,
        provider: llmManager.getCurrentProviderId()
      });
    }

  } catch (error) {
    logger.error("[LLM API] ‚ùå Erreur:", error);
    return NextResponse.json(
      { error: "Erreur interne du serveur" },
      { status: 500 }
    );
  }
} 