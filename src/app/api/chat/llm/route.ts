import { NextRequest, NextResponse } from 'next/server';
import { createClient } from '@supabase/supabase-js';
import { LLMProviderManager } from '@/services/llm/providerManager';
import { DeepSeekProvider, TogetherProvider } from '@/services/llm/providers';
// Import temporairement d√©sactiv√© pour r√©soudre le probl√®me de build Vercel
import { agentApiV2Tools } from '@/services/agentApiV2Tools';

import type { AppContext, ChatMessage } from '@/services/llm/types';
import { simpleLogger as logger } from '@/utils/logger';

// Instance singleton du LLM Manager
const llmManager = new LLMProviderManager();

// Configuration Supabase - V√©rification diff√©r√©e pour √©viter les erreurs de build
const getSupabaseConfig = () => {
  const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
  const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

  if (!supabaseUrl || !supabaseServiceKey) {
    throw new Error('Variables d\'environnement Supabase manquantes');
  }

  return { supabaseUrl, supabaseServiceKey };
};

// Fonction pour cr√©er le client admin
const createSupabaseAdmin = () => {
  const { supabaseUrl, supabaseServiceKey } = getSupabaseConfig();
  return createClient(supabaseUrl, supabaseServiceKey);
};

// Fonction pour r√©cup√©rer un agent
const getAgentById = async (id: string) => {
  try {
    const supabase = createSupabaseAdmin();
    const { data, error } = await supabase
      .from('agents')
      .select('*')
      .eq('id', id)
      .single();

    if (error) {
      logger.error('Erreur lors de la r√©cup√©ration de l\'agent:', error);
      return null;
    }

    return data;
  } catch (error) {
    logger.error('Erreur getAgentById:', error);
    return null;
  }
};

// Fonction pour r√©cup√©rer l'historique des messages d'une session
const getSessionHistory = async (sessionId: string, userToken: string) => {
  try {
    const { supabaseUrl, supabaseServiceKey } = getSupabaseConfig();
    const userClient = createClient(supabaseUrl, supabaseServiceKey, {
      global: {
        headers: {
          Authorization: `Bearer ${userToken}`
        }
      }
    });

    // R√©cup√©rer la session avec le thread complet
    const { data: session, error } = await userClient
      .from('chat_sessions')
      .select('thread, history_limit')
      .eq('id', sessionId)
      .single();

    if (error) {
      logger.error('[LLM API] ‚ùå Erreur r√©cup√©ration session:', error);
      return [];
    }

    if (!session) {
      logger.error('[LLM API] ‚ùå Session non trouv√©e:', sessionId);
      return [];
    }

    // Appliquer la limite d'historique
    const historyLimit = session.history_limit || 10;
    const limitedHistory = (session.thread || []).slice(-historyLimit);

    logger.dev('[LLM API] üìö Historique r√©cup√©r√©:', {
      sessionId,
      totalMessages: session.thread?.length || 0,
      limitedMessages: limitedHistory.length,
      limit: historyLimit
    });

    return limitedHistory;
  } catch (error) {
    logger.error('[LLM API] ‚ùå Erreur getSessionHistory:', error);
    return [];
  }
};

/**
 * Nettoie et valide les arguments JSON des function calls
 */
const cleanAndParseFunctionArgs = (rawArgs: string): any => {
  try {
    // Essayer de parser directement
    return JSON.parse(rawArgs);
  } catch (error) {
    logger.dev("[LLM API] ‚ö†Ô∏è Arguments JSON malform√©s, tentative de nettoyage:", rawArgs);
    
    try {
      // Nettoyer les arguments en supprimant les caract√®res probl√©matiques
      let cleanedArgs = rawArgs
        .replace(/\n/g, '') // Supprimer les retours √† la ligne
        .replace(/\r/g, '') // Supprimer les retours chariot
        .replace(/\t/g, '') // Supprimer les tabulations
        .trim();
      
      // Si on a plusieurs objets JSON concat√©n√©s, prendre le premier
      if (cleanedArgs.includes('}{')) {
        const firstBrace = cleanedArgs.indexOf('{');
        const lastBrace = cleanedArgs.lastIndexOf('}');
        if (firstBrace !== -1 && lastBrace !== -1) {
          cleanedArgs = cleanedArgs.substring(firstBrace, lastBrace + 1);
        }
      }
      
      // Essayer de parser le JSON nettoy√©
      const parsed = JSON.parse(cleanedArgs);
      logger.dev("[LLM API] ‚úÖ Arguments nettoy√©s avec succ√®s:", parsed);
      return parsed;
      
    } catch (cleanError) {
      logger.error("[LLM API] ‚ùå Impossible de nettoyer les arguments JSON:", cleanError);
      throw new Error(`Arguments JSON invalides: ${rawArgs}`);
    }
  }
};

export async function POST(request: NextRequest) {
  logger.dev("[LLM API] üöÄ REQU√äTE RE√áUE !");
  try {
    // V√©rifier l'authentification
    const authHeader = request.headers.get('authorization');
    let userId: string;
    let userToken: string;

    if (authHeader && authHeader.startsWith('Bearer ')) {
      userToken = authHeader.substring(7);
      const supabase = createSupabaseAdmin();
      const { data: { user }, error: authError } = await supabase.auth.getUser(userToken);
      
      if (authError || !user) {
        logger.dev("[LLM API] ‚ùå Token invalide ou expir√©");
        return NextResponse.json(
          { error: 'Token invalide ou expir√©' },
          { status: 401 }
        );
      }
      userId = user.id;
      logger.dev("[LLM API] ‚úÖ Utilisateur authentifi√©:", userId);
    } else {
      logger.dev("[LLM API] ‚ùå Token d'authentification manquant");
      return NextResponse.json(
        { error: 'Authentification requise' },
        { status: 401 }
      );
    }

    const { message, context, history, provider, channelId: incomingChannelId } = await request.json();
    
    logger.dev("[LLM API] üöÄ D√©but de la requ√™te");
    logger.dev("[LLM API] üë§ Utilisateur:", userId);
    logger.dev("[LLM API] üì¶ Body re√ßu:", { message, context, provider });

    // üîß CORRECTION: R√©cup√©rer l'historique depuis la base de donn√©es
    let sessionHistory: ChatMessage[] = [];
    if (context?.sessionId) {
      logger.dev("[LLM API] üìö R√©cup√©ration historique pour session:", context.sessionId);
      sessionHistory = await getSessionHistory(context.sessionId, userToken);
      logger.dev("[LLM API] ‚úÖ Historique r√©cup√©r√©:", sessionHistory.length, "messages");
      
      // üîß CORRECTION: Exclure le dernier message s'il correspond au message actuel
      if (sessionHistory.length > 0) {
        const lastMessage = sessionHistory[sessionHistory.length - 1];
        if (lastMessage.content === message && lastMessage.role === 'user') {
          logger.dev("[LLM API] üîß Exclusion du dernier message (d√©j√† dans l'historique):", message);
          sessionHistory = sessionHistory.slice(0, -1);
          logger.dev("[LLM API] ‚úÖ Historique corrig√©:", sessionHistory.length, "messages");
        }
      }
    } else {
      logger.dev("[LLM API] ‚ö†Ô∏è Pas de sessionId, utilisation de l'historique fourni");
      sessionHistory = history || [];
    }

    // R√©cup√©rer la configuration de l'agent si sp√©cifi√©e
    let agentConfig: any = null;
    if (context?.agentId) {
      logger.dev("[LLM API] üîç Recherche agent avec ID:", context.agentId);
      agentConfig = await getAgentById(context.agentId);
      logger.dev("[LLM API] ü§ñ Configuration agent trouv√©e:", agentConfig?.name);
      if (agentConfig) {
        if (agentConfig.system_instructions) {
          logger.dev("[LLM API] üìù Instructions syst√®me (extrait):", agentConfig.system_instructions.substring(0, 200) + '...');
        }
      } else {
        logger.dev("[LLM API] ‚ö†Ô∏è Agent non trouv√© pour l'ID:", context.agentId);
      }
    } else {
      logger.dev("[LLM API] ‚ö†Ô∏è Aucun agentId fourni dans le contexte");
    }

    // D√©terminer le provider √† utiliser
    let targetProvider = provider;
    
    // PRIORIT√â 1: Agent s√©lectionn√© (priorit√© absolue)
    if (agentConfig && agentConfig.provider) {
      targetProvider = agentConfig.provider;
      logger.dev("[LLM API] üéØ Agent s√©lectionn√© - Forcer provider:", agentConfig.provider, "pour l'agent:", agentConfig.name);
    }
    // PRIORIT√â 2: Provider manuel (menu kebab)
    else if (provider) {
      targetProvider = provider;
      logger.dev("[LLM API] üîß Provider manuel s√©lectionn√©:", provider);
    }
    // PRIORIT√â 3: Provider par d√©faut
    else {
      targetProvider = 'synesia';
      logger.dev("[LLM API] ‚öôÔ∏è Utilisation du provider par d√©faut:", targetProvider);
    }

    // Changer de provider si n√©cessaire
    if (targetProvider && targetProvider !== llmManager.getCurrentProviderId()) {
      llmManager.setProvider(targetProvider);
      logger.dev("[LLM API] üîÑ Provider chang√© vers:", targetProvider);
    }

    // Pr√©parer le contexte par d√©faut si non fourni
    const appContext: AppContext = context || {
      type: 'chat_session',
      id: 'default',
      name: 'Chat g√©n√©ral'
    };

    // Utiliser le provider manager
    const currentProvider = llmManager.getCurrentProvider();
    if (!currentProvider) {
      throw new Error('Aucun provider LLM disponible');
    }

    logger.dev("[LLM API] üöÄ Provider utilis√©:", currentProvider.id);

    // V√©rifier si c'est DeepSeek pour le streaming
    if (currentProvider.id === 'deepseek') {
      logger.dev("[LLM API] üöÄ Streaming avec DeepSeek");
      
      // Cr√©er un canal unique pour le streaming
      const channelId = incomingChannelId || `llm-stream-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      logger.dev("[LLM API] üì° Canal utilis√©:", channelId);
      
      // Utiliser le provider avec configuration d'agent
      const deepseekProvider = new DeepSeekProvider();
      logger.dev("[LLM API] üîß Configuration avant merge:", {
        defaultModel: deepseekProvider.getDefaultConfig().model,
        defaultInstructions: deepseekProvider.getDefaultConfig().system_instructions?.substring(0, 50) + '...'
      });
      
      const config = deepseekProvider['mergeConfigWithAgent'](agentConfig || undefined);
      logger.dev("[LLM API] üîß Configuration apr√®s merge:", {
        model: config.model,
        temperature: config.temperature,
        instructions: config.system_instructions?.substring(0, 100) + '...'
      });
      
      // Pr√©parer les messages avec la configuration dynamique
      const systemContent = deepseekProvider['formatContext'](appContext, config);
      logger.dev("[LLM API] üìù Contenu syst√®me pr√©par√©:", systemContent.substring(0, 200) + '...');
      
      const messages = [
        {
          role: 'system' as const,
          content: systemContent
        },
        ...sessionHistory.map((msg: ChatMessage) => ({
          role: msg.role as 'user' | 'assistant' | 'system',
          content: msg.content
        })),
        {
          role: 'user' as const,
          content: message
        }
      ];

      // üîß TOOLS: G√©n√©rer les outils pour function calling
      const tools = agentApiV2Tools.getToolsForFunctionCalling();

      logger.dev("[LLM API] üîß Capacit√©s agent:", agentConfig?.api_v2_capabilities);
              logger.dev("[LLM API] üîß Tools disponibles:", tools?.length || 0);

      // Appeler DeepSeek avec streaming et configuration dynamique
      const payload = {
        model: config.model,
        messages,
        stream: true,
        temperature: config.temperature,
        max_tokens: config.max_tokens,
        top_p: config.top_p,
        ...(tools && { tools })
      };

      logger.dev("[LLM API] üì§ Payload complet envoy√© √† DeepSeek:");
      logger.dev(JSON.stringify(payload, null, 2));
      logger.dev("[LLM API] üì§ Appel DeepSeek avec streaming");

      const response = await fetch('https://api.deepseek.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`
        },
        body: JSON.stringify(payload)
      });

      if (!response.ok) {
        const errorText = await response.text();
        logger.error("[LLM API] ‚ùå Erreur DeepSeek:", errorText);
        throw new Error(`DeepSeek API error: ${response.status} - ${errorText}`);
      }

      // Gestion du streaming avec function calling
      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('Impossible de lire le stream de r√©ponse');
      }

      let accumulatedContent = '';
      let functionCallData: any = null;
      let tokenBuffer = '';
      let bufferSize = 0;
      const BATCH_SIZE = 5; // Envoyer par batch de 5 tokens
      const BATCH_TIMEOUT = 100; // Ou toutes les 100ms

      // Cr√©er le canal pour le broadcast
      const supabase = createSupabaseAdmin();
      const channel = supabase.channel(channelId);

      // Fonction pour envoyer le buffer de tokens
      const flushTokenBuffer = async () => {
        if (tokenBuffer.length > 0) {
          await channel.send({
            type: 'broadcast',
            event: 'llm-token-batch',
            payload: {
              tokens: tokenBuffer,
              sessionId: context.sessionId
            }
          });
          tokenBuffer = '';
          bufferSize = 0;
        }
      };

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = new TextDecoder().decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') break;

            try {
              const parsed = JSON.parse(data);
              
              logger.dev("[LLM API] üì• Chunk complet:", JSON.stringify(parsed));
              
              if (parsed.choices?.[0]?.delta) {
                const delta = parsed.choices[0].delta;
                logger.dev("[LLM API] üîç Delta trouv√©:", JSON.stringify(delta));
                
                // Gestion du function calling (ancien format)
                if (delta.function_call) {
                  if (!functionCallData) {
                    functionCallData = {
                      name: delta.function_call.name || '',
                      arguments: delta.function_call.arguments || ''
                    };
                  } else {
                    if (delta.function_call.name) {
                      functionCallData.name = delta.function_call.name;
                    }
                    if (delta.function_call.arguments) {
                      functionCallData.arguments += delta.function_call.arguments;
                    }
                  }
                }
                // Gestion du tool calling (nouveau format)
                else if (delta.tool_calls) {
                  logger.dev("[LLM API] üîß Tool calls d√©tect√©s:", JSON.stringify(delta.tool_calls));
                  
                  for (const toolCall of delta.tool_calls) {
                    if (!functionCallData) {
                      functionCallData = {
                        name: toolCall.function?.name || '',
                        arguments: toolCall.function?.arguments || ''
                      };
                    } else {
                      if (toolCall.function?.name) {
                        functionCallData.name = toolCall.function.name;
                      }
                      if (toolCall.function?.arguments) {
                        functionCallData.arguments += toolCall.function.arguments;
                      }
                    }
                  }
                }
                else if (delta.content) {
                  accumulatedContent += delta.content;
                  tokenBuffer += delta.content;
                  bufferSize++;
                  
                  // Log √©pur√© pour le streaming
                  logger.dev("[LLM API] üì§ Token ajout√© au buffer:", bufferSize);
                  
                  // Envoyer le buffer si on atteint la taille ou le timeout
                  if (bufferSize >= BATCH_SIZE) {
                    try {
                      await flushTokenBuffer();
                      logger.dev("[LLM API] üì¶ Batch envoy√©");
                    } catch (error) {
                      logger.error("[LLM API] ‚ùå Erreur broadcast batch:", error);
                    }
                  }
                }
              }
            } catch (error) {
              logger.error("[LLM API] ‚ùå Erreur parsing chunk:", error);
            }
          }
        }
      }

              // Envoyer le buffer restant
        await flushTokenBuffer();

        // Si une fonction a √©t√© appel√©e, l'ex√©cuter
        logger.dev("[LLM API] üîç Function call d√©tect√©e:", functionCallData);
        
        // üîß ANTI-BOUCLE: Limiter √† une seule ex√©cution de fonction par requ√™te
        if (functionCallData && functionCallData.name) {
        logger.dev("[LLM API] üöÄ Ex√©cution tool:", functionCallData.name);
        try {
          // üîß NOUVEAU: Nettoyer et valider les arguments JSON
          const functionArgs = cleanAndParseFunctionArgs(functionCallData.arguments);
          
          // Timeout de 15 secondes pour les tool calls
          const toolCallPromise = agentApiV2Tools.executeTool(
            functionCallData.name, 
            functionArgs, 
            userToken
          );
          
          const timeoutPromise = new Promise((_, reject) => {
            setTimeout(() => reject(new Error('Timeout tool call (15s)')), 15000);
          });
          
          const result = await Promise.race([toolCallPromise, timeoutPromise]);

          logger.dev("[LLM API] ‚úÖ Tool ex√©cut√©:", result);

          // üîß CORRECTION: Injecter le message tool et relancer le LLM
          logger.dev("[LLM API] üîß Injection du message tool et relance LLM");

          // 1. Cr√©er le message tool avec le bon format
          const toolCallId = `call_${Date.now()}`;
          const toolMessage = {
            role: 'assistant' as const,
            content: null,
            tool_calls: [{
              id: toolCallId,
              type: 'function',
              function: {
                name: functionCallData.name,
                arguments: functionCallData.arguments
              }
            }]
          };

          const toolResultMessage = {
            role: 'tool' as const,
            tool_call_id: toolCallId,
            content: JSON.stringify(result)
          };

          // 2. Ajouter les messages √† l'historique
          const updatedMessages = [
            ...messages,
            toolMessage,
            toolResultMessage
          ];

          // üîß NOUVEAU: Sauvegarder les messages tool dans la base de donn√©es
          try {
            // Utiliser directement l'API avec le token utilisateur
            const response1 = await fetch(`${process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:3000'}/api/v1/chat-sessions/${context.sessionId}/messages`, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${userToken}`
              },
              body: JSON.stringify({
                role: 'assistant',
                content: null,
                tool_calls: [{
                  id: toolCallId,
                  type: 'function',
                  function: {
                    name: functionCallData.name,
                    arguments: functionCallData.arguments
                  }
                }],
                timestamp: new Date().toISOString()
              })
            });

            if (!response1.ok) {
              throw new Error(`Erreur sauvegarde message assistant: ${response1.status}`);
            }

            // Sauvegarder le message tool avec le r√©sultat
            const response2 = await fetch(`${process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:3000'}/api/v1/chat-sessions/${context.sessionId}/messages`, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${userToken}`
              },
              body: JSON.stringify({
                role: 'tool',
                tool_call_id: toolCallId,
                content: JSON.stringify(result),
                timestamp: new Date().toISOString()
              })
            });

            if (!response2.ok) {
              throw new Error(`Erreur sauvegarde message tool: ${response2.status}`);
            }

            logger.dev("[LLM API] ‚úÖ Messages tool sauvegard√©s dans l'historique");
          } catch (saveError) {
            logger.error("[LLM API] ‚ùå Erreur sauvegarde messages tool:", saveError);
            // Continuer m√™me si la sauvegarde √©choue
          }

          // 3. Relancer le LLM avec l'historique complet (SANS tools pour √©viter la boucle infinie)
          const finalPayload = {
            model: config.model,
            messages: updatedMessages,
            stream: true,
            temperature: config.temperature,
            max_tokens: config.max_tokens,
            top_p: config.top_p
            // üîß ANTI-BOUCLE: Pas de tools lors de la relance
          };

          logger.dev("[LLM API] üîÑ Relance LLM avec payload:", JSON.stringify(finalPayload, null, 2));

          const finalResponse = await fetch('https://api.deepseek.com/v1/chat/completions', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`
            },
            body: JSON.stringify(finalPayload)
          });

          if (!finalResponse.ok) {
            const errorText = await finalResponse.text();
            logger.error("[LLM API] ‚ùå Erreur DeepSeek relance:", errorText);
            throw new Error(`DeepSeek API error: ${finalResponse.status} - ${errorText}`);
          }

          logger.dev("[LLM API] üîÑ LLM relanc√© avec historique complet");

          // 4. Streamer la vraie r√©ponse du LLM
          const encoder = new TextEncoder();
          const stream = new ReadableStream({
            async start(controller) {
              try {
                const reader = finalResponse.body?.getReader();
                if (!reader) {
                  throw new Error('Impossible de lire le stream de r√©ponse finale');
                }

                let accumulatedContent = '';
                let isComplete = false;

                while (true) {
                  const { done, value } = await reader.read();
                  if (done) break;

                  const chunk = new TextDecoder().decode(value);
                  const lines = chunk.split('\n');

                  for (const line of lines) {
                    if (line.startsWith('data: ')) {
                      const data = line.slice(6);
                      if (data === '[DONE]') {
                        isComplete = true;
                        break;
                      }

                      try {
                        const parsed = JSON.parse(data);
                        const delta = parsed.choices?.[0]?.delta;
                        
                        if (delta?.content) {
                          const token = delta.content;
                          accumulatedContent += token;
                          
                          // Broadcast du token
                          await channel.send({
                            type: 'broadcast',
                            event: 'llm-token',
                            payload: {
                              token,
                              sessionId: context.sessionId
                            }
                          });

                          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ token })}\n\n`));
                        }
                      } catch (parseError) {
                        logger.dev("[LLM API] ‚ö†Ô∏è Chunk non-JSON ignor√©:", data);
                      }
                    }
                  }

                  if (isComplete) break;
                }

                // Broadcast de completion avec le contenu accumul√©
                await channel.send({
                  type: 'broadcast',
                  event: 'llm-complete',
                  payload: {
                    sessionId: context.sessionId,
                    fullResponse: accumulatedContent
                  }
                });

                logger.dev("[LLM API] ‚úÖ Streaming termin√©, contenu accumul√©:", accumulatedContent.substring(0, 100) + "...");

                controller.close();
              } catch (error) {
                logger.error("[LLM API] ‚ùå Erreur streaming r√©ponse finale:", error);
                controller.error(error);
              }
            }
          });

          return new Response(stream, {
            headers: {
              'Content-Type': 'text/plain; charset=utf-8',
              'Cache-Control': 'no-cache',
              'Connection': 'keep-alive',
            },
          });

        } catch (error) {
          logger.error("[LLM API] ‚ùå Erreur ex√©cution fonction:", error);
          
          const errorMessage = `Erreur lors de l'ex√©cution de l'action: ${error instanceof Error ? error.message : 'Erreur inconnue'}`;
          
          // üîß CORRECTION: Injecter l'erreur dans l'historique et relancer le LLM
          logger.dev("[LLM API] üîß Injection de l'erreur tool dans l'historique");

          // 1. Cr√©er le message tool avec l'erreur
          const toolCallId = `call_${Date.now()}`;
          const toolMessage = {
            role: 'assistant' as const,
            content: null,
            tool_calls: [{
              id: toolCallId,
              type: 'function',
              function: {
                name: functionCallData.name,
                arguments: functionCallData.arguments
              }
            }]
          };

          const toolResultMessage = {
            role: 'tool' as const,
            tool_call_id: toolCallId,
            content: JSON.stringify({ 
              error: true, 
              message: errorMessage 
            })
          };

          // 2. Ajouter les messages √† l'historique
          const updatedMessages = [
            ...messages,
            toolMessage,
            toolResultMessage
          ];

          // üîß NOUVEAU: Sauvegarder les messages tool dans la base de donn√©es
          try {
            const { ChatSessionService } = await import('@/services/chatSessionService');
            const chatSessionService = ChatSessionService.getInstance();
            
            // Sauvegarder le message assistant avec tool call
            await chatSessionService.addMessage(context.sessionId, {
              role: 'assistant',
              content: null,
              tool_calls: [{
                id: toolCallId,
                type: 'function',
                function: {
                  name: functionCallData.name,
                  arguments: functionCallData.arguments
                }
              }],
              timestamp: new Date().toISOString()
            });

            // Sauvegarder le message tool avec le r√©sultat
            await chatSessionService.addMessage(context.sessionId, {
              role: 'tool',
              tool_call_id: toolCallId,
              content: JSON.stringify({ 
                error: true, 
                message: `‚ùå √âCHEC : ${errorMessage}`,
                success: false,
                action: 'failed'
              }),
              timestamp: new Date().toISOString()
            });

            logger.dev("[LLM API] ‚úÖ Messages tool sauvegard√©s dans l'historique");
          } catch (saveError) {
            logger.error("[LLM API] ‚ùå Erreur sauvegarde messages tool:", saveError);
            // Continuer m√™me si la sauvegarde √©choue
          }

          // 3. Relancer le LLM avec l'historique complet (SANS tools)
          const finalPayload = {
            model: config.model,
            messages: updatedMessages,
            stream: true,
            temperature: config.temperature,
            max_tokens: config.max_tokens,
            top_p: config.top_p
            // üîß ANTI-BOUCLE: Pas de tools lors de la relance
          };

          logger.dev("[LLM API] üîÑ Relance LLM avec erreur tool");

          const finalResponse = await fetch('https://api.deepseek.com/v1/chat/completions', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`
            },
            body: JSON.stringify(finalPayload)
          });

          if (!finalResponse.ok) {
            const errorText = await finalResponse.text();
            logger.error("[LLM API] ‚ùå Erreur DeepSeek relance:", errorText);
            throw new Error(`DeepSeek API error: ${finalResponse.status} - ${errorText}`);
          }

          logger.dev("[LLM API] üîÑ LLM relanc√© avec erreur tool");

          // 4. Streamer la r√©ponse du LLM avec l'erreur
          const encoder = new TextEncoder();
          const stream = new ReadableStream({
            async start(controller) {
              try {
                const reader = finalResponse.body?.getReader();
                if (!reader) {
                  throw new Error('Impossible de lire le stream de r√©ponse finale');
                }

                let accumulatedContent = '';
                let isComplete = false;

                while (true) {
                  const { done, value } = await reader.read();
                  if (done) break;

                  const chunk = new TextDecoder().decode(value);
                  const lines = chunk.split('\n');

                  for (const line of lines) {
                    if (line.startsWith('data: ')) {
                      const data = line.slice(6);
                      if (data === '[DONE]') {
                        isComplete = true;
                        break;
                      }

                      try {
                        const parsed = JSON.parse(data);
                        const delta = parsed.choices?.[0]?.delta;
                        
                        if (delta?.content) {
                          const token = delta.content;
                          accumulatedContent += token;
                          
                          // Broadcast du token
                          await channel.send({
                            type: 'broadcast',
                            event: 'llm-token',
                            payload: {
                              token,
                              sessionId: context.sessionId
                            }
                          });

                          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ token })}\n\n`));
                        }
                      } catch (parseError) {
                        logger.dev("[LLM API] ‚ö†Ô∏è Chunk non-JSON ignor√©:", data);
                      }
                    }
                  }

                  if (isComplete) break;
                }

                // Broadcast de completion avec le contenu accumul√©
                await channel.send({
                  type: 'broadcast',
                  event: 'llm-complete',
                  payload: {
                    sessionId: context.sessionId,
                    fullResponse: accumulatedContent
                  }
                });

                logger.dev("[LLM API] ‚úÖ Streaming termin√© avec erreur tool, contenu accumul√©:", accumulatedContent.substring(0, 100) + "...");

                controller.close();
              } catch (streamError) {
                logger.error("[LLM API] ‚ùå Erreur streaming r√©ponse finale:", streamError);
                controller.error(streamError);
              }
            }
          });

          return new Response(stream, {
            headers: {
              'Content-Type': 'text/plain; charset=utf-8',
              'Cache-Control': 'no-cache',
              'Connection': 'keep-alive',
            },
          });
        }
      } else {
        logger.dev("[LLM API] ‚ùå PAS DE FUNCTION CALL - R√©ponse normale");
        // R√©ponse normale sans function calling
        // Broadcast de completion
        await channel.send({
          type: 'broadcast',
          event: 'llm-complete',
          payload: {
            sessionId: context.sessionId,
            fullResponse: accumulatedContent
          }
        });
        
        // üîß CORRECTION: Retourner du JSON pur pour √©viter l'erreur parsing
        return NextResponse.json({ 
          success: true, 
          completed: true,
          response: accumulatedContent 
        });
      }

    } else {
      // Pour les autres providers (Synesia, Together AI, etc.)
      if (currentProvider.id === 'together') {
        logger.dev("[LLM API] üöÄ Streaming avec Together AI");
        
        // Cr√©er un canal unique pour le streaming
        const channelId = incomingChannelId || `llm-stream-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
        logger.dev("[LLM API] üì° Canal utilis√©:", channelId);
        
        // Utiliser le provider avec configuration d'agent
        const togetherProvider = new TogetherProvider();
        logger.dev("[LLM API] üîß Configuration avant merge:", {
          defaultModel: togetherProvider.getDefaultConfig().model,
          defaultInstructions: togetherProvider.getDefaultConfig().system_instructions?.substring(0, 50) + '...'
        });
        
        const config = togetherProvider['mergeConfigWithAgent'](agentConfig || undefined);
        logger.dev("[LLM API] üîß Configuration apr√®s merge:", {
          model: config.model,
          temperature: config.temperature,
          instructions: config.system_instructions?.substring(0, 100) + '...'
        });
        
        // Pr√©parer les messages avec la configuration dynamique
        const systemContent = togetherProvider['formatContext'](appContext, config);
        logger.dev("[LLM API] üìù Contenu syst√®me pr√©par√©:", systemContent.substring(0, 200) + '...');
        
        const messages = [
          {
            role: 'system' as const,
            content: systemContent
          },
          ...sessionHistory.map((msg: ChatMessage) => ({
            role: msg.role as 'user' | 'assistant' | 'system',
            content: msg.content
          })),
          {
            role: 'user' as const,
            content: message
          }
        ];

        // Appeler Together AI avec streaming
        const payload = {
          model: config.model,
          messages,
          stream: true,
          temperature: config.temperature,
          max_tokens: config.max_tokens,
          top_p: config.top_p
        };

        logger.dev("[LLM API] üì§ Payload complet envoy√© √† Together AI:");
        logger.dev(JSON.stringify(payload, null, 2));
        logger.dev("[LLM API] üì§ Appel Together AI avec streaming");

        const response = await fetch('https://api.together.xyz/v1/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${process.env.TOGETHER_API_KEY}`
          },
          body: JSON.stringify(payload)
        });

        if (!response.ok) {
          const errorText = await response.text();
          logger.error("[LLM API] ‚ùå Erreur Together AI:", errorText);
          throw new Error(`Together AI API error: ${response.status} - ${errorText}`);
        }

        // Gestion du streaming
        const reader = response.body?.getReader();
        if (!reader) {
          throw new Error('Impossible de lire le stream de r√©ponse');
        }

        let accumulatedContent = '';
        let tokenBuffer = '';
        let bufferSize = 0;
        const BATCH_SIZE = 5; // Envoyer par batch de 5 tokens

        // Cr√©er le canal pour le broadcast
        const supabase = createSupabaseAdmin();
        const channel = supabase.channel(channelId);

        // Fonction pour envoyer le buffer de tokens
        const flushTokenBuffer = async () => {
          if (tokenBuffer.length > 0) {
            await channel.send({
              type: 'broadcast',
              event: 'llm-token-batch',
              payload: {
                tokens: tokenBuffer,
                sessionId: context.sessionId
              }
            });
            tokenBuffer = '';
            bufferSize = 0;
          }
        };

        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          const chunk = new TextDecoder().decode(value);
          const lines = chunk.split('\n');

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6);
              if (data === '[DONE]') break;

              try {
                const parsed = JSON.parse(data);
                const delta = parsed.choices?.[0]?.delta;
                
                if (delta?.content) {
                  const token = delta.content;
                  accumulatedContent += token;
                  tokenBuffer += token;
                  bufferSize++;
                  
                  // Envoyer le buffer si on atteint la taille
                  if (bufferSize >= BATCH_SIZE) {
                    try {
                      await flushTokenBuffer();
                      logger.dev("[LLM API] üì¶ Batch Together AI envoy√©");
                    } catch (error) {
                      logger.error("[LLM API] ‚ùå Erreur broadcast batch Together AI:", error);
                    }
                  }
                }
              } catch (parseError) {
                logger.dev("[LLM API] ‚ö†Ô∏è Chunk non-JSON ignor√©:", data);
              }
            }
          }
        }

        // Broadcast de completion avec le contenu accumul√©
        await channel.send({
          type: 'broadcast',
          event: 'llm-complete',
          payload: {
            sessionId: context.sessionId,
            fullResponse: accumulatedContent
          }
        });

        logger.dev("[LLM API] ‚úÖ Streaming Together AI termin√©, contenu accumul√©:", accumulatedContent.substring(0, 100) + "...");

        // Retourner du JSON pur pour √©viter l'erreur parsing
        return NextResponse.json({ 
          success: true, 
          completed: true,
          response: accumulatedContent 
        });
      } else {
        // Pour les autres providers (Synesia, etc.)
        const response = await currentProvider.call(message, appContext, sessionHistory);
        return NextResponse.json({ success: true, response });
      }
    }

  } catch (error) {
    logger.error("[LLM API] ‚ùå Erreur g√©n√©rale:", error);
    return NextResponse.json(
      { error: 'Erreur interne du serveur' },
      { status: 500 }
    );
  }
} 