/**
 * Provider Cerebras - API Inference
 * 
 * Impl√©mentation compl√®te avec support:
 * - Streaming SSE
 * - Tool calls (function calling)
 * - Reasoning (si support√© par le mod√®le)
 * 
 * Documentation: https://inference-docs.cerebras.ai
 */

import { BaseProvider, type ProviderConfig, type ProviderInfo } from '../base/BaseProvider';
import type { LLMProvider, AppContext } from '../../types';
import type { ChatMessage } from '@/types/chat';
import type { LLMResponse, ToolCall, Tool } from '../../types/strictTypes';
import { simpleLogger as logger } from '@/utils/logger';
import { getSystemMessage } from '../../templates';
import type {
  CerebrasMessage,
  CerebrasChatCompletionRequest,
  CerebrasChatCompletionResponse,
  CerebrasStreamChunk
} from '../../types/cerebrasTypes';
import { isFunctionTool } from '../../types/strictTypes';

/**
 * Type pour les chunks de streaming SSE
 */
interface StreamChunk {
  type?: 'delta';
  content?: string;
  tool_calls?: ToolCall[];
  finishReason?: 'stop' | 'length' | 'tool_calls' | 'content_filter' | null;
  reasoning?: string;
  usage?: Usage;
}

interface Usage {
  prompt_tokens?: number;
  completion_tokens?: number;
  total_tokens?: number;
}

/**
 * Configuration sp√©cifique √† Cerebras
 */
interface CerebrasConfig extends ProviderConfig {
  // Sp√©cifique √† Cerebras
  parallelToolCalls?: boolean; // ‚úÖ Support des appels parall√®les (d√©faut: false)
  strictToolCalls?: boolean; // ‚úÖ Mode strict pour tool calls (d√©faut: false)
  clearThinking?: boolean; // ‚úÖ zai-glm-4.7: Exclure le thinking pr√©c√©dent du contexte (d√©faut: true)
  reasoningEffort?: 'low' | 'medium' | 'high'; // ‚úÖ gpt-oss-120b: Niveau de reasoning (d√©faut: medium)
  minTokens?: number; // ‚úÖ gpt-oss-120b: Minimum de tokens √† g√©n√©rer (peut causer des EOS tokens)
}

/**
 * Informations sur Cerebras
 */
const CEREBRAS_INFO: ProviderInfo = {
  id: 'cerebras',
  name: 'Cerebras',
  version: '1.0.0',
  description: 'Ultra-fast inference platform with Llama and other models',
  capabilities: {
    functionCalls: true,
    streaming: true,
    reasoning: true, // ‚úÖ Support√© par zai-glm-4.7 et autres mod√®les r√©cents
    codeExecution: false,
    webSearch: false,
    structuredOutput: true // ‚úÖ Support√© par zai-glm-4.7 avec strict: true
  },
  supportedModels: [
    'zai-glm-4.7', // ‚úÖ Nouveau mod√®le avec reasoning, tool calling, structured outputs
    'gpt-oss-120b', // ‚úÖ OpenAI GPT OSS - Reasoning avanc√©, tr√®s rapide (~3000 tokens/sec)
    'llama-3.3-70b',
    'llama-3.1-8b',
    'llama-3.1-70b'
  ],
  pricing: {
    input: '$2.25 / M tokens', // ‚úÖ Pricing zai-glm-4.7
    output: '$2.75 / M tokens' // ‚úÖ Pricing zai-glm-4.7
  }
};

/**
 * Rotation des cl√©s API Cerebras (round-robin)
 */
class CerebrasApiKeyRotator {
  private static instance: CerebrasApiKeyRotator;
  private apiKeys: string[] = [];
  private currentIndex = 0;

  private constructor() {
    // R√©cup√©rer toutes les cl√©s disponibles depuis les variables d'environnement
    const keys: string[] = [];
    if (process.env.CEREBRAS_API_KEY) {
      keys.push(process.env.CEREBRAS_API_KEY);
    }
    if (process.env.CEREBRAS_API_KEY_2) {
      keys.push(process.env.CEREBRAS_API_KEY_2);
    }
    // Support pour plus de cl√©s si n√©cessaire
    let keyIndex = 3;
    while (process.env[`CEREBRAS_API_KEY_${keyIndex}`]) {
      keys.push(process.env[`CEREBRAS_API_KEY_${keyIndex}`] as string);
      keyIndex++;
    }

    this.apiKeys = keys.filter(key => key && key.trim().length > 0);

    if (this.apiKeys.length === 0) {
      logger.warn('[CerebrasApiKeyRotator] ‚ö†Ô∏è Aucune cl√© API Cerebras trouv√©e');
    } else {
      logger.info(`[CerebrasApiKeyRotator] ‚úÖ ${this.apiKeys.length} cl√©(s) API configur√©e(s) pour rotation`);
    }
  }

  static getInstance(): CerebrasApiKeyRotator {
    if (!CerebrasApiKeyRotator.instance) {
      CerebrasApiKeyRotator.instance = new CerebrasApiKeyRotator();
    }
    return CerebrasApiKeyRotator.instance;
  }

  /**
   * Obtient la prochaine cl√© API (round-robin)
   */
  getNextApiKey(): string {
    if (this.apiKeys.length === 0) {
      return '';
    }

    const key = this.apiKeys[this.currentIndex];
    this.currentIndex = (this.currentIndex + 1) % this.apiKeys.length;
    
    return key;
  }

  /**
   * Obtient toutes les cl√©s disponibles
   */
  getAllKeys(): string[] {
    return [...this.apiKeys];
  }

  /**
   * V√©rifie si au moins une cl√© est disponible
   */
  hasKeys(): boolean {
    return this.apiKeys.length > 0;
  }
}

/**
 * Configuration par d√©faut de Cerebras
 */
const DEFAULT_CEREBRAS_CONFIG: CerebrasConfig = {
  // Base
  apiKey: process.env.CEREBRAS_API_KEY || '',
  baseUrl: 'https://api.cerebras.ai/v1',
  timeout: 120000, // 120s
  
  // LLM
  model: 'zai-glm-4.7', // ‚úÖ Mod√®le par d√©faut avec reasoning et tool calling avanc√©
  temperature: 0.7,
  maxTokens: 40000, // ‚úÖ Max output pour zai-glm-4.7 (40k tokens)
  topP: 0.9,
  
  // Features
  supportsFunctionCalls: true,
  supportsStreaming: false, // Streaming g√©r√© par la route API
  supportsReasoning: true, // ‚úÖ zai-glm-4.7 supporte le reasoning (activ√© par d√©faut)
  
  // Monitoring
  enableLogging: true,
  enableMetrics: true,
  
  // Cerebras sp√©cifique
  parallelToolCalls: false, // ‚úÖ D√©sactiv√© par d√©faut (comme Groq)
  strictToolCalls: false, // ‚úÖ Mode strict d√©sactiv√© par d√©faut
  clearThinking: true, // ‚úÖ zai-glm-4.7: Exclure le thinking pr√©c√©dent par d√©faut
  reasoningEffort: 'medium' // ‚úÖ gpt-oss-120b: Niveau de reasoning par d√©faut (medium)
};

/**
 * Provider Cerebras pour l'API Cerebras
 */
export class CerebrasProvider extends BaseProvider implements LLMProvider {
  readonly info = CEREBRAS_INFO;
  readonly config: CerebrasConfig;

  // Impl√©mentation de LLMProvider
  get name(): string {
    return this.info.name;
  }

  get id(): string {
    return this.info.id;
  }

  constructor(customConfig?: Partial<CerebrasConfig>) {
    super();
    this.config = { ...DEFAULT_CEREBRAS_CONFIG, ...customConfig };
  }

  /**
   * V√©rifie si Cerebras est disponible
   */
  isAvailable(): boolean {
    return this.validateConfig();
  }

  /**
   * Valide la configuration de Cerebras
   */
  validateConfig(): boolean {
    if (!this.validateBaseConfig()) {
      logger.error('[CerebrasProvider] ‚ùå Configuration de base invalide');
      return false;
    }

    if (!this.config.model) {
      logger.error('[CerebrasProvider] ‚ùå Mod√®le non sp√©cifi√©');
      return false;
    }

    if (!this.info.supportedModels.includes(this.config.model)) {
      logger.warn(`[CerebrasProvider] ‚ö†Ô∏è Mod√®le ${this.config.model} non officiellement support√©`);
    }

    logger.dev('[CerebrasProvider] ‚úÖ Configuration valid√©e');
    return true;
  }

  /**
   * Effectue un appel √† l'API Cerebras avec support des function calls
   */
  async call(message: string, context: AppContext, history: ChatMessage[]): Promise<LLMResponse> {
    if (!this.isAvailable()) {
      throw new Error('Cerebras provider non configur√©');
    }

    try {
      logger.dev(`[CerebrasProvider] üöÄ Appel avec mod√®le: ${this.config.model}`);

      // ‚úÖ V√©rifier si le streaming est activ√©
      if (this.config.supportsStreaming) {
        throw new Error('Streaming non support√© dans le provider Cerebras - utilisez la route API directement');
      }

      // Pr√©parer les messages
      const messages = this.prepareMessages(message, context, history);
      
      // Pr√©parer le payload (sans streaming)
      const payload = await this.preparePayload(messages, []);
      payload.stream = false; // Forcer le mode non-streaming
      
      // Effectuer l'appel API
      const response = await this.makeApiCall(payload);
      
      // Extraire la r√©ponse
      const result = this.extractResponse(response);
      
      logger.dev('[CerebrasProvider] ‚úÖ Appel r√©ussi');
      
      return {
        content: result.content || '',
        tool_calls: result.tool_calls || [],
        model: result.model,
        usage: result.usage,
        reasoning: result.reasoning
      };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      const stack = error instanceof Error ? error.stack : 'No stack trace';
      
      logger.error('[CerebrasProvider] ‚ùå Erreur lors de l\'appel:', {
        message: errorMessage,
        stack: stack,
        rawError: error
      });
      
      if (error instanceof Error) {
        throw error;
      } else {
        throw new Error(`Erreur inattendue dans CerebrasProvider: ${errorMessage}`);
      }
    }
  }

  /**
   * Effectue un appel √† l'API Cerebras avec une liste de messages d√©j√† pr√©par√©e
   */
  async callWithMessages(messages: ChatMessage[], tools: Tool[]): Promise<LLMResponse> {
    if (!this.isAvailable()) {
      throw new Error('Cerebras provider non configur√©');
    }

    try {
      logger.info(`[CerebrasProvider] üöÄ Appel Chat Completions avec ${messages.length} messages`);
      
      // Conversion des ChatMessage vers le format API
      const apiMessages = this.convertChatMessagesToApiFormat(messages);
      const payload = await this.preparePayload(apiMessages, tools);
      payload.stream = false;
      
      const messagesCount = Array.isArray(payload.messages) ? payload.messages.length : 0;
      const toolsCount = Array.isArray(payload.tools) ? payload.tools.length : 0;
      logger.info(`[CerebrasProvider] ‚Üí Chat Completions: ${payload.model} | ${messagesCount} msgs | ${toolsCount} tools`);
      
      const response = await this.makeApiCall(payload);
      const result = this.extractResponse(response);
      
      logger.dev('[CerebrasProvider] ‚úÖ Appel Chat Completions r√©ussi');
      
      return {
        content: result.content || '',
        tool_calls: result.tool_calls || [],
        model: result.model,
        usage: result.usage,
        reasoning: result.reasoning
      };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      logger.error('[CerebrasProvider] ‚ùå Erreur lors de l\'appel:', { message: errorMessage });
      throw error;
    }
  }

  /**
   * ‚úÖ Streaming avec Server-Sent Events (SSE)
   * Compatible avec Cerebras API (format OpenAI)
   */
  async *callWithMessagesStream(
    messages: ChatMessage[], 
    tools: Tool[]
  ): AsyncGenerator<StreamChunk, void, unknown> {
    if (!this.isAvailable()) {
      throw new Error('Cerebras provider non configur√©');
    }

    try {
      logger.dev(`[CerebrasProvider] üåä Streaming Chat Completions avec ${messages.length} messages`);
      
      // Conversion des ChatMessage vers le format API
      const apiMessages = this.convertChatMessagesToApiFormat(messages);
      const payload = await this.preparePayload(apiMessages, tools);
      payload.stream = true; // ‚úÖ Activer streaming
      
      const messageCount = Array.isArray(payload.messages) ? payload.messages.length : 0;
      const toolsCount = Array.isArray(payload.tools) ? payload.tools.length : 0;
      logger.info(`[CerebrasProvider] üöÄ PAYLOAD ‚Üí CEREBRAS: ${payload.model} | ${messageCount} messages | ${toolsCount} tools`);
      
      // ‚úÖ Rotation automatique des cl√©s API
      const rotator = CerebrasApiKeyRotator.getInstance();
      const apiKey = rotator.hasKeys() ? rotator.getNextApiKey() : this.config.apiKey;

      // Appel API avec streaming
      const response = await fetch(`${this.config.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        },
        body: JSON.stringify(payload)
      });

      if (!response.ok) {
        const errorText = await response.text();
        
        // ‚úÖ Parser le body JSON si possible
        let errorDetails: { error?: { message?: string; type?: string; code?: string } } = {};
        try {
          errorDetails = JSON.parse(errorText);
        } catch {
          // Si le parsing √©choue, on garde errorText brut
        }
        
        const errorMessage = errorDetails.error?.message || errorText;
        const errorCode = errorDetails.error?.code || errorDetails.error?.type || 'unknown';
        
        logger.error(`[CerebrasProvider] ‚ùå Erreur API Cerebras Streaming:`, {
          statusCode: response.status,
          statusText: response.statusText,
          errorMessage,
          errorCode,
          model: this.config.model,
          messagesCount: messages.length,
          toolsCount: tools.length
        });
        
        const error = new Error(`Cerebras API error: ${response.status} - ${errorMessage}`);
        (error as Error & { statusCode?: number; provider?: string; errorCode?: string }).statusCode = response.status;
        (error as Error & { statusCode?: number; provider?: string; errorCode?: string }).provider = 'cerebras';
        (error as Error & { statusCode?: number; provider?: string; errorCode?: string }).errorCode = errorCode;
        throw error;
      }

      if (!response.body) {
        throw new Error('Response body is null');
      }

      // Lire le stream SSE
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        
        if (done) {
          logger.dev(`[CerebrasProvider] ‚úÖ Stream termin√©`);
          break;
        }

        // D√©coder le chunk
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          const trimmed = line.trim();
          
          if (!trimmed || trimmed === 'data: [DONE]') continue;
          if (!trimmed.startsWith('data: ')) continue;

          try {
            const jsonStr = trimmed.substring(6);
            const chunk = JSON.parse(jsonStr) as CerebrasStreamChunk;
            
            const delta = chunk.choices?.[0]?.delta;
            if (!delta) continue;

            // ‚úÖ Construire le chunk (format identique √† Groq/xAI)
            const streamChunk: StreamChunk = {
              type: 'delta'
            };
            
            if (delta.content) {
              streamChunk.content = delta.content;
            }
            
            if (delta.tool_calls && delta.tool_calls.length > 0) {
              streamChunk.tool_calls = delta.tool_calls
                .filter((tc): tc is NonNullable<typeof tc> => tc !== null && tc !== undefined)
                .map((tc) => ({
                  id: tc.id || '',
                  type: 'function' as const,
                  function: {
                    name: tc.function?.name || '',
                    arguments: tc.function?.arguments || ''
                  }
                }));
            }
            
            // ‚úÖ Reasoning (si support√© par le mod√®le)
            if (delta.reasoning) {
              streamChunk.reasoning = delta.reasoning;
            }
            
            if (chunk.choices?.[0]?.finish_reason) {
              streamChunk.finishReason = chunk.choices[0].finish_reason;
            }

            if (chunk.usage) {
              streamChunk.usage = chunk.usage;
            }

            // ‚úÖ Yield tous les chunks
            yield streamChunk;
            
          } catch (parseError) {
            logger.error('[CerebrasProvider] ‚ùå Erreur parsing chunk SSE:', parseError);
          }
        }
      }

    } catch (error) {
      logger.error('[CerebrasProvider] ‚ùå Erreur streaming:', error);
      throw error;
    }
  }

  /**
   * Convertit les ChatMessage vers le format API Cerebras
   * ‚ö†Ô∏è CRITIQUE: Cerebras REQUIS qu'un message 'tool' soit pr√©c√©d√© d'un message 'assistant' avec 'tool_calls'
   */
  private convertChatMessagesToApiFormat(messages: ChatMessage[]): CerebrasMessage[] {
    const result: CerebrasMessage[] = [];
    
    for (let i = 0; i < messages.length; i++) {
      const msg = messages[i];
      
      // ‚úÖ G√©rer les messages assistant
      if (msg.role === 'assistant') {
        const assistantMsg = msg as import('@/types/chat').AssistantMessage;
        
        // ‚úÖ CRITIQUE: Si tool_calls existe, v√©rifier s'ils sont r√©solus
        if (assistantMsg.tool_calls && assistantMsg.tool_calls.length > 0) {
          // ‚úÖ Cas 1: tool_results pr√©sents dans le champ ‚Üí tool_calls r√©solus
          if (assistantMsg.tool_results && assistantMsg.tool_results.length > 0) {
            // ‚úÖ Envoyer assistant SANS tool_calls (les tool_results suivent)
            const assistantMessage: CerebrasMessage = {
              role: 'assistant',
              content: typeof msg.content === 'string' ? msg.content : null
            };
            result.push(assistantMessage);
            
            // ‚úÖ Transformer tool_results en messages tool
            for (const toolResult of assistantMsg.tool_results) {
              result.push({
                role: 'tool',
                tool_call_id: toolResult.tool_call_id,
                content: typeof toolResult.content === 'string' 
                  ? toolResult.content 
                  : JSON.stringify(toolResult.content ?? null)
              });
            }
            
            logger.dev(`[CerebrasProvider] ‚úÖ Message assistant avec ${assistantMsg.tool_results.length} tool_results (dans champ)`);
            continue;
          }
          
          // ‚úÖ Cas 2: Pas de tool_results dans le champ ‚Üí v√©rifier s'il y a des messages tool qui suivent
          const followingToolMessages: ChatMessage[] = [];
          let j = i + 1;
          while (j < messages.length && messages[j].role === 'tool') {
            followingToolMessages.push(messages[j]);
            j++;
          }
          
          // ‚úÖ Si des messages tool suivent ‚Üí tool_calls r√©solus (messages tool s√©par√©s)
          if (followingToolMessages.length > 0) {
            // ‚úÖ Envoyer assistant AVEC tool_calls
            const assistantMessage: CerebrasMessage = {
              role: 'assistant',
              content: typeof msg.content === 'string' ? msg.content : null,
              tool_calls: assistantMsg.tool_calls.map(tc => ({
                id: tc.id,
                type: 'function' as const,
                function: {
                  name: tc.function?.name || '',
                  arguments: tc.function?.arguments || ''
                }
              }))
            };
            result.push(assistantMessage);
            
            // ‚úÖ Envoyer les messages tool qui suivent
            for (const toolMsg of followingToolMessages) {
              if (toolMsg.role !== 'tool') {
                continue;
              }
              
              const toolMessage = toolMsg as import('@/types/chat').ToolMessage;
              const toolCallId = toolMessage.tool_call_id;
              if (!toolCallId) {
                logger.warn(`[CerebrasProvider] ‚ö†Ô∏è Message tool sans tool_call_id, SKIP`);
                continue;
              }
              
              result.push({
                role: 'tool',
                tool_call_id: toolCallId,
                content: typeof toolMessage.content === 'string' 
                  ? toolMessage.content 
                  : (typeof toolMessage.content === 'object' && toolMessage.content !== null 
                    ? JSON.stringify(toolMessage.content) 
                    : String(toolMessage.content ?? ''))
              });
            }
            
            // ‚úÖ Avancer l'index pour skip les messages tool qu'on vient de traiter
            i = j - 1; // -1 car le for va incr√©menter
            
            logger.dev(`[CerebrasProvider] ‚úÖ Message assistant avec ${assistantMsg.tool_calls.length} tool_calls + ${followingToolMessages.length} messages tool s√©par√©s`);
            continue;
          }
          
          // ‚úÖ Cas 3: Pas de tool_results ET pas de messages tool suivants ‚Üí SKIP (appel en cours)
          logger.warn(`[CerebrasProvider] ‚ö†Ô∏è SKIP message assistant avec ${assistantMsg.tool_calls.length} tool_calls non r√©solus (pas de tool_results ni messages tool suivants)`);
          continue;
        }
        
        // ‚úÖ Message assistant sans tool_calls : envoyer normalement
        const assistantMessage: CerebrasMessage = {
          role: 'assistant',
          content: typeof msg.content === 'string' ? msg.content : null
        };
        result.push(assistantMessage);
        continue;
      }
      
      // ‚úÖ G√©rer les messages tool : uniquement s'ils ne sont pas d√©j√† trait√©s (apr√®s un assistant)
      // Les messages tool sont normalement trait√©s avec leur assistant pr√©c√©dent
      // Mais on peut avoir des messages tool orphelins (sans assistant pr√©c√©dent) ‚Üí on les skip
      if (msg.role === 'tool') {
        // ‚úÖ V√©rifier si le message pr√©c√©dent dans result est un assistant avec tool_calls
        const lastResult = result[result.length - 1];
        const isAfterAssistantWithToolCalls = lastResult?.role === 'assistant' && lastResult.tool_calls && lastResult.tool_calls.length > 0;
        
        if (!isAfterAssistantWithToolCalls) {
          logger.warn(`[CerebrasProvider] ‚ö†Ô∏è SKIP message tool orphelin (pas d'assistant avec tool_calls pr√©c√©dent)`);
          continue;
        }
        // Sinon, le message tool a d√©j√† √©t√© trait√© avec son assistant pr√©c√©dent
        continue;
      }
      
      // ‚úÖ Messages user et system : envoyer normalement
      const messageObj: CerebrasMessage = {
        role: msg.role as 'user' | 'system',
        content: typeof msg.content === 'string' ? msg.content : null
      };
      result.push(messageObj);
    }
    
    return result;
  }

  /**
   * Pr√©pare les messages pour l'API
   * ‚ö†Ô∏è Utilise convertChatMessagesToApiFormat pour g√©rer correctement tool_calls et tool messages
   */
  private prepareMessages(message: string, context: AppContext, history: ChatMessage[]): CerebrasMessage[] {
    const messages: CerebrasMessage[] = [];

    // Message syst√®me avec contexte
    const systemContent = this.formatSystemMessage(context);
    messages.push({
      role: 'system',
      content: systemContent
    });

    // ‚úÖ Utiliser convertChatMessagesToApiFormat pour g√©rer correctement tool_calls et tool messages
    // Cette m√©thode g√®re d√©j√† les tool_results et les messages tool s√©par√©s
    const historyMessages = this.convertChatMessagesToApiFormat(history);
    messages.push(...historyMessages);

    // Message utilisateur actuel
    messages.push({
      role: 'user',
      content: message
    });

    return messages;
  }

  /**
   * Pr√©pare le payload pour l'API Cerebras avec support des tools
   */
  private async preparePayload(messages: CerebrasMessage[], tools: Tool[]): Promise<CerebrasChatCompletionRequest> {
    // ‚úÖ Nettoyer les messages pour Cerebras (format conforme √† la documentation)
    const cleanedMessages = messages.map(msg => ({
      role: msg.role,
      content: msg.content,
      ...(msg.tool_calls && { tool_calls: msg.tool_calls }),
      ...(msg.tool_call_id && { tool_call_id: msg.tool_call_id })
      // ‚úÖ Note: Cerebras n'utilise PAS le champ 'name' pour les messages tool
    }));

    const payload: CerebrasChatCompletionRequest = {
      model: this.config.model,
      messages: cleanedMessages,
      temperature: this.config.temperature,
      max_tokens: this.config.maxTokens,
      top_p: this.config.topP,
      stream: false
    };
    
    // ‚úÖ zai-glm-4.7: Ajouter clear_thinking si configur√©
    // Contr√¥le si le thinking pr√©c√©dent est inclus dans le contexte (d√©faut: true = exclure)
    // Documentation: https://inference-docs.cerebras.ai/models/zai-glm-47
    if (this.config.clearThinking !== undefined) {
      payload.clear_thinking = this.config.clearThinking;
    }
    
    // ‚úÖ gpt-oss-120b: Ajouter reasoning_effort si configur√©
    // ‚ö†Ô∏è IMPORTANT: reasoning_effort est UNIQUEMENT pour gpt-oss-120b, PAS pour zai-glm-4.7
    // zai-glm-4.7 a le reasoning activ√© par d√©faut (pas de param√®tre reasoning_effort)
    // Documentation: https://inference-docs.cerebras.ai/models/openai-oss
    if (this.config.reasoningEffort && this.config.model === 'gpt-oss-120b') {
      payload.reasoning_effort = this.config.reasoningEffort;
    }
    
    // ‚úÖ gpt-oss-120b: Ajouter min_tokens si configur√©
    // ‚ö†Ô∏è ATTENTION: Peut causer des tokens EOS et des erreurs de parsing
    if (this.config.minTokens !== undefined) {
      payload.min_tokens = this.config.minTokens;
      logger.warn('[CerebrasProvider] ‚ö†Ô∏è min_tokens activ√© - peut causer des tokens EOS');
    }

    if (tools && tools.length > 0) {
      // ‚úÖ Convertir les tools au format Cerebras avec support strict mode
      payload.tools = tools
        .filter((tool): tool is Extract<Tool, { type: 'function' }> => isFunctionTool(tool))
        .map(tool => {
          const toolDef: NonNullable<CerebrasChatCompletionRequest['tools']>[number] = {
            type: 'function' as const,
            function: {
              name: tool.function.name,
              description: tool.function.description,
              parameters: {
                ...tool.function.parameters,
                // ‚úÖ En mode strict, additionalProperties doit √™tre false
                ...(this.config.strictToolCalls && {
                  additionalProperties: false
                })
              }
            }
          };
          
          // ‚úÖ Ajouter strict: true si activ√© dans la config
          if (this.config.strictToolCalls) {
            toolDef.function.strict = true;
          }
          
          return toolDef;
        });
      payload.tool_choice = 'auto';
      
      // ‚úÖ Ajouter parallel_tool_calls selon la config
      payload.parallel_tool_calls = this.config.parallelToolCalls ?? false;
    }
    
    return payload;
  }

  /**
   * Effectue l'appel API √† Cerebras
   */
  private async makeApiCall(payload: CerebrasChatCompletionRequest): Promise<CerebrasChatCompletionResponse> {
    // ‚úÖ Rotation automatique des cl√©s API
    const rotator = CerebrasApiKeyRotator.getInstance();
    const apiKey = rotator.hasKeys() ? rotator.getNextApiKey() : this.config.apiKey;

    const response = await fetch(`${this.config.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(payload),
      signal: AbortSignal.timeout(this.config.timeout)
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Cerebras API error: ${response.status} - ${errorText}`);
    }

    if (payload.stream) {
      throw new Error('Streaming non support√© dans makeApiCall - utilisez callWithMessagesStream');
    }

    const responseData = await response.json() as CerebrasChatCompletionResponse;
    logger.dev('[CerebrasProvider] üîç R√©ponse brute de l\'API Cerebras:', {
      hasChoices: 'choices' in responseData,
      choices: responseData?.choices,
      hasContent: responseData?.choices?.[0]?.message?.content
    });
    
    return responseData;
  }

  /**
   * Extrait la r√©ponse de l'API Cerebras avec support des tool calls
   */
  private extractResponse(response: CerebrasChatCompletionResponse): LLMResponse {
    if (!response.choices || response.choices.length === 0) {
      throw new Error('R√©ponse invalide de Cerebras API');
    }

    const choice = response.choices[0];
    const result: LLMResponse = {
      content: choice?.message?.content ?? '',
      model: response.model,
      usage: response.usage
    };

    // ‚úÖ Ajouter les tool calls si pr√©sents
    if (choice?.message?.tool_calls && choice.message.tool_calls.length > 0) {
      result.tool_calls = choice.message.tool_calls;
      logger.dev(`[CerebrasProvider] üîß ${result.tool_calls.length} tool calls d√©tect√©s`);
    }

    // ‚úÖ Ajouter le reasoning si pr√©sent
    if (choice?.message?.reasoning) {
      result.reasoning = choice.message.reasoning;
      logger.dev(`[CerebrasProvider] üß† Reasoning d√©tect√©`);
    }

    return result;
  }

  /**
   * Formate le message syst√®me avec le contexte
   */
  private formatSystemMessage(context: AppContext): string {
    if (context.content && context.content.trim().length > 0) {
      logger.dev(`[CerebrasProvider] üéØ Utilisation des instructions syst√®me fournies`);
      return context.content;
    }

    const message = getSystemMessage('assistant-contextual', { context });
    if (!message) {
      return 'Tu es un assistant IA utile et bienveillant.';
    }
    
    logger.dev(`[CerebrasProvider] ‚öôÔ∏è Utilisation du template par d√©faut`);
    return message;
  }

  /**
   * Retourne les tools disponibles pour les function calls
   */
  getFunctionCallTools(): Tool[] {
    // Retourner un tableau vide temporairement
    return [];
  }
}
