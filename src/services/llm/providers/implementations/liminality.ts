/**
 * Provider Liminality - Synesia LLM Exec API
 * 
 * Int√©gration de l'API Synesia LLM Exec avec support complet des tools avanc√©s
 * (callable, knowledge, openapi, mcp) et orchestration automatique multi-tours.
 * 
 * Architecture:
 * - H√©rite de BaseProvider pour la structure commune
 * - Impl√©mente LLMProvider avec call(), callWithMessages(), callWithMessagesStream()
 * - Utilise LiminalityToolsAdapter pour convertir les tools Groq/xAI
 * - Support streaming SSE avec events riches (chunk, tool_call, tool_result)
 * - Orchestration automatique via config.max_loops
 */

import { BaseProvider, type ProviderConfig, type ProviderInfo } from '../base/BaseProvider';
import type { LLMProvider, AppContext } from '../../types';
import type { ChatMessage } from '@/types/chat';
import type { LLMResponse, ToolCall, Tool } from '../../types/strictTypes';
import { simpleLogger as logger } from '@/utils/logger';
import { getSystemMessage } from '../../templates';
import { LiminalityToolsAdapter } from '../adapters/LiminalityToolsAdapter';
import type { 
  LiminalityTool, 
  LiminalityLLMConfig, 
  LiminalityOrchestrationConfig,
  LiminalityMessage,
  LiminalityResponse,
  LiminalityStreamEvent,
  LiminalityRequestPayload,
  LiminalityToolCallInMessage
} from '../../types/liminalityTypes';

/**
 * Configuration sp√©cifique √† Liminality
 */
interface LiminalityProviderConfig extends ProviderConfig {
  maxLoops?: number;
}

/**
 * Usage tokens d'un appel LLM
 */
interface TokenUsage {
  prompt_tokens?: number;
  completion_tokens?: number;
  total_tokens?: number;
}

/**
 * Type pour les chunks de streaming SSE
 */
interface StreamChunk {
  type?: 'delta';
  content?: string;
  tool_calls?: ToolCall[];
  finishReason?: 'stop' | 'length' | 'tool_calls' | 'content_filter' | null;
  reasoning?: string;
  usage?: TokenUsage; // ‚úÖ TYP√â au lieu de unknown
}

/**
 * Informations sur le provider Liminality
 */
const LIMINALITY_INFO: ProviderInfo = {
  id: 'liminality',
  name: 'Liminality',
  version: '1.0.0',
  description: 'Synesia LLM Exec API with advanced tools orchestration',
  capabilities: {
    functionCalls: true,
    streaming: true,
    reasoning: true,
    codeExecution: true,
    webSearch: true,
    structuredOutput: true
  },
  supportedModels: [
    'gpt-4o-mini',
    'gpt-4o',
    'claude-3-haiku',
    'claude-3-sonnet',
    'claude-3-5-sonnet',
    'groq-llama-3-70b',
    'deepseek-chat',
    'fireworks/z-ai-glm-4-7',
    'openrouter/mimo-v2-flash:free',
    'openrouter/kimi-k2-thinking',
    'openrouter/glm-4.7',
    'openrouter/gemini-3-flash-preview',
    'openrouter/minimax-m2.1',
    'openrouter/qwen3-vl-30b-a3b-instruct'
  ],
  pricing: {
    input: 'Variable (depends on underlying model)',
    output: 'Variable (depends on underlying model)'
  }
};

/**
 * Configuration par d√©faut de Liminality
 */
const DEFAULT_CONFIG: LiminalityProviderConfig = {
  apiKey: process.env.LIMINALITY_API_KEY || '',
  baseUrl: 'https://origins-server.up.railway.app',
  timeout: 120000, // 120s (2 minutes) - permet tool calls longs et orchestration
  model: 'gpt-4o-mini',
  temperature: 0.7,
  maxTokens: 8000,
  topP: 0.9,
  supportsFunctionCalls: true,
  supportsStreaming: true,
  supportsReasoning: true,
  enableLogging: true,
  enableMetrics: true,
  maxLoops: 10 // Orchestration automatique par d√©faut
};

/**
 * Provider Liminality pour l'API Synesia LLM Exec
 */
export class LiminalityProvider extends BaseProvider implements LLMProvider {
  readonly info = LIMINALITY_INFO;
  readonly config: LiminalityProviderConfig;

  // Impl√©mentation de LLMProvider
  get name(): string {
    return this.info.name;
  }

  get id(): string {
    return this.info.id;
  }

  constructor(customConfig?: Partial<LiminalityProviderConfig>) {
    super();
    this.config = { ...DEFAULT_CONFIG, ...customConfig };
    
    // üîç DEBUG: V√©rifier la cl√© API
    if (!this.config.apiKey) {
      logger.error('[LiminalityProvider] ‚ùå API Key manquante! V√©rifiez LIMINALITY_API_KEY dans .env.local');
    } else {
      logger.dev(`[LiminalityProvider] ‚úÖ API Key charg√©e: ${this.config.apiKey.substring(0, 15)}...`);
    }
  }

  /**
   * V√©rifie si Liminality est disponible
   */
  isAvailable(): boolean {
    return this.validateConfig();
  }

  /**
   * Valide la configuration de Liminality
   */
  validateConfig(): boolean {
    if (!this.validateBaseConfig()) {
      logger.error('[LiminalityProvider] ‚ùå Configuration de base invalide');
      return false;
    }

    if (!this.config.model) {
      logger.error('[LiminalityProvider] ‚ùå Mod√®le non sp√©cifi√©');
      return false;
    }

    if (!this.info.supportedModels.includes(this.config.model)) {
      logger.warn(`[LiminalityProvider] ‚ö†Ô∏è Mod√®le ${this.config.model} non officiellement support√©`);
    }

    logger.dev('[LiminalityProvider] ‚úÖ Configuration valid√©e');
    return true;
  }

  /**
   * Effectue un appel √† l'API Liminality (d√©l√®gue √† callWithMessages)
   */
  async call(message: string, context: AppContext, history: ChatMessage[]): Promise<LLMResponse> {
    if (!this.isAvailable()) {
      throw new Error('Liminality provider non configur√©');
    }

    try {
      logger.dev(`[LiminalityProvider] üöÄ Appel avec mod√®le: ${this.config.model}`);

      // Pr√©parer les messages
      const messages = this.prepareMessages(message, context, history);
      
      // Appeler avec les messages pr√©par√©s
      return await this.callWithMessages(messages, []);

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      const stack = error instanceof Error ? error.stack : 'No stack trace';
      
      logger.error('[LiminalityProvider] ‚ùå Erreur lors de l\'appel:', {
        message: errorMessage,
        stack: stack,
        rawError: error
      });
      
      if (error instanceof Error) {
        throw error;
      } else {
        throw new Error(`Erreur inattendue dans LiminalityProvider: ${errorMessage}`);
      }
    }
  }

  /**
   * Effectue un appel √† l'API Liminality avec une liste de messages d√©j√† pr√©par√©e
   */
  async callWithMessages(messages: ChatMessage[], tools: Tool[], synesiaCallables?: string[]): Promise<LLMResponse> {
    if (!this.isAvailable()) {
      throw new Error('Liminality provider non configur√©');
    }

    try {
      logger.dev(`[LiminalityProvider] üöÄ Appel avec ${messages.length} messages`);
      
      // Conversion des ChatMessage vers le format API Liminality
      const apiMessages = this.convertChatMessagesToApiFormat(messages);
      
      // Conversion des tools via l'adapter
      let liminalityTools = LiminalityToolsAdapter.convert(tools);
      
      // Ajouter les callables Synesia si fournis
      if (synesiaCallables && synesiaCallables.length > 0) {
        liminalityTools = LiminalityToolsAdapter.addSynesiaTools(liminalityTools, {
          callables: synesiaCallables,
        });
        logger.info(`[LiminalityProvider] üîó ${synesiaCallables.length} callables ajout√©s aux tools`);
      }
      
      // Pr√©parer le payload
      const payload = this.preparePayload(apiMessages, liminalityTools);
      
      logger.info(`[LiminalityProvider] üöÄ PAYLOAD ‚Üí LIMINALITY: ${payload.model} | ${apiMessages.length} messages | ${liminalityTools.length} tools`);
      
      // Appel API
      const response = await this.makeApiCall(payload);
      
      // Extraire la r√©ponse
      const result = this.extractResponse(response);
      
      logger.dev('[LiminalityProvider] ‚úÖ Appel r√©ussi');
      
      return {
        content: result.content || '',
        tool_calls: result.tool_calls || [],
        model: this.config.model,
        usage: result.usage,
        reasoning: result.reasoning
      };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      logger.error('[LiminalityProvider] ‚ùå Erreur lors de l\'appel:', { message: errorMessage });
      throw error;
    }
  }

  /**
   * Streaming avec Server-Sent Events (SSE)
   */
  async *callWithMessagesStream(
    messages: ChatMessage[], 
    tools: Tool[],
    synesiaCallables?: string[]
  ): AsyncGenerator<StreamChunk, void, unknown> {
    if (!this.isAvailable()) {
      throw new Error('Liminality provider non configur√©');
    }

    try {
      logger.dev(`[LiminalityProvider] üåä Streaming avec ${messages.length} messages`);
      
      // Conversion des ChatMessage vers le format API
      const apiMessages = this.convertChatMessagesToApiFormat(messages);
      
      // Conversion des tools via l'adapter
      let liminalityTools = LiminalityToolsAdapter.convert(tools);
      
      // Ajouter les callables Synesia si fournis
      if (synesiaCallables && synesiaCallables.length > 0) {
        liminalityTools = LiminalityToolsAdapter.addSynesiaTools(liminalityTools, {
          callables: synesiaCallables,
        });
        logger.info(`[LiminalityProvider] üîó ${synesiaCallables.length} callables ajout√©s aux tools`);
      }
      
      // Pr√©parer le payload
      const payload = this.preparePayload(apiMessages, liminalityTools);
      
      logger.info(`[LiminalityProvider] üöÄ Stream call: ${payload.model} | ${apiMessages.length} messages | ${liminalityTools.length} tools`);
      
      // Appel API avec streaming
      const response = await fetch(`${this.config.baseUrl}/llm-exec/round/stream`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': this.config.apiKey
        },
        body: JSON.stringify(payload)
      });

      if (!response.ok) {
        const errorText = await response.text();
        
        // Parser le body JSON si possible
        let errorDetails: { error?: string; statusCode?: number } = {};
        try {
          errorDetails = JSON.parse(errorText);
        } catch {
          // Si le parsing √©choue, on garde errorText brut
        }
        
        const errorMessage = errorDetails.error || errorText;
        
        logger.error(`[LiminalityProvider] ‚ùå Erreur API Liminality Streaming:`, {
          statusCode: response.status,
          statusText: response.statusText,
          errorText: errorText,
          errorDetails: errorDetails,
          errorMessage,
          model: this.config.model,
          messagesCount: messages.length,
          toolsCount: tools.length,
          url: `${this.config.baseUrl}/llm-exec/round/stream`
        });
        
        const error = new Error(`Liminality API error: ${response.status} - ${errorMessage}`);
        (error as Error & { statusCode?: number; provider?: string }).statusCode = response.status;
        (error as Error & { statusCode?: number; provider?: string }).provider = 'liminality';
        throw error;
      }

      if (!response.body) {
        throw new Error('Response body is null');
      }

      // Lire le stream SSE
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;

        // D√©coder le chunk
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || ''; // Garder la derni√®re ligne incompl√®te

        for (const line of lines) {
          const trimmed = line.trim();
          
          // Ignorer les lignes vides et les commentaires
          if (!trimmed || trimmed.startsWith(':')) {
            continue;
          }

          // Parser les lignes SSE (format "data: {...}")
          if (trimmed.startsWith('data: ')) {
            const data = trimmed.slice(6); // Enlever "data: "
            
            if (data === '[DONE]') break;

            try {
              // ‚úÖ VALIDATION : V√©rifier taille avant parsing (s√©curit√©)
              const MAX_EVENT_SIZE = 10 * 1024 * 1024; // 10MB max
              if (data.length > MAX_EVENT_SIZE) {
                logger.error('[LiminalityProvider] ‚ùå Event trop volumineux', {
                  size: data.length,
                  maxSize: MAX_EVENT_SIZE
                });
                continue;
              }
              
              const event = JSON.parse(data) as LiminalityStreamEvent;
              const chunk = this.convertStreamEvent(event);
              
              if (chunk) {
                yield chunk;
              }

            } catch (parseError) {
              // ‚úÖ ERROR HANDLING : Logger avec contexte complet
              logger.error('[LiminalityProvider] ‚ùå Erreur parsing SSE event', {
                error: parseError instanceof Error ? parseError.message : String(parseError),
                dataPreview: data.substring(0, 200), // Premiers 200 caract√®res
                dataLength: data.length
              });
              continue; // Ignorer l'event invalide et continuer le stream
            }
          }
        }
      }

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      logger.error('[LiminalityProvider] ‚ùå Erreur streaming:', { message: errorMessage });
      throw error;
    }
  }

  /**
   * Pr√©pare les messages avec le system message
   */
  private prepareMessages(message: string, context: AppContext, history: ChatMessage[]): ChatMessage[] {
    const systemMessage = getSystemMessage('assistant-contextual', { context });
    
    const messages: ChatMessage[] = [
      {
        id: 'system',
        role: 'system',
        content: systemMessage
      } as ChatMessage,
      ...history.slice(-10), // Garder les 10 derniers messages
      {
        id: `user-${Date.now()}`,
        role: 'user',
        content: message
      } as ChatMessage
    ];
    
    return messages;
  }

  /**
   * Convertit les ChatMessage vers le format API Liminality
   */
  private convertChatMessagesToApiFormat(messages: ChatMessage[]): LiminalityMessage[] {
    return messages.map((msg) => {
      const limMsg: LiminalityMessage = {
        role: this.mapRole(msg.role),
        content: msg.content || ''
      };

      // Ajouter tool_calls si pr√©sents (assistant messages uniquement)
      if (msg.role === 'assistant') {
        const assistantMsg = msg as import('@/types/chat').AssistantMessage;
        if (assistantMsg.tool_calls && assistantMsg.tool_calls.length > 0) {
          // ‚úÖ ERROR HANDLING : Parser avec try/catch pour chaque tool call
          limMsg.tool_calls = assistantMsg.tool_calls.map(tc => {
            let parsedArguments: Record<string, unknown> = {};
            
            if (typeof tc.function?.arguments === 'string') {
              try {
                parsedArguments = JSON.parse(tc.function.arguments);
              } catch (parseError) {
                logger.warn('[LiminalityProvider] ‚ö†Ô∏è Erreur parsing tool call arguments', {
                  toolCallId: tc.id,
                  toolName: tc.function?.name,
                  error: parseError instanceof Error ? parseError.message : String(parseError),
                  argumentsPreview: tc.function.arguments.substring(0, 100)
                });
                parsedArguments = {}; // Fallback : arguments vides
              }
            } else if (tc.function?.arguments && typeof tc.function.arguments === 'object') {
              parsedArguments = tc.function.arguments as Record<string, unknown>;
            }
            
            return {
              id: tc.id,
              name: tc.function?.name || '',
              arguments: parsedArguments
            };
          });
        }

        // Ajouter reasoning si pr√©sent
        if (assistantMsg.reasoning) {
          limMsg.reasoning = assistantMsg.reasoning;
        }
      }

      // Formatter correctement les tool_response messages selon le format Synesia
      if (msg.role === 'tool') {
        const toolMsg = msg as import('@/types/chat').ToolMessage;
        
        if (toolMsg.tool_call_id) {
          // Format Synesia : tool_calls en array
          limMsg.tool_calls = [{
            tool_call_id: toolMsg.tool_call_id,
            content: limMsg.content,
            tool_name: toolMsg.name
          }];
          delete limMsg.content;
        } else {
          logger.error(`[LiminalityProvider] ‚ùå Tool message sans tool_call_id`);
        }
      }

      return limMsg;
    });
  }

  /**
   * Mappe les r√¥les ChatMessage vers les r√¥les Liminality
   */
  private mapRole(role: string): LiminalityMessage['role'] {
    switch (role) {
      case 'user':
        return 'user';
      case 'assistant':
        return 'assistant';
      case 'system':
        return 'system';
      case 'tool':
        return 'tool_response';
      default:
        return 'user';
    }
  }

  /**
   * Pr√©pare le payload pour l'API Liminality
   */
  private preparePayload(
    messages: LiminalityMessage[],
    tools: LiminalityTool[]
  ): LiminalityRequestPayload {
    const llmConfig: LiminalityLLMConfig = {
      temperature: this.config.temperature,
      max_completion_tokens: this.config.maxTokens,
      top_p: this.config.topP,
      tool_choice: 'auto',
      parallel_tool_calls: false // D√©sactiv√© pour √©viter les probl√®mes
    };

    const orchestrationConfig: LiminalityOrchestrationConfig = {
      max_loops: this.config.maxLoops || 10
    };

    const payload: LiminalityRequestPayload = {
      model: this.config.model,
      messages,
      llmConfig,
      config: orchestrationConfig
    };

    // Ajouter les tools si pr√©sents
    if (tools && tools.length > 0) {
      payload.tools = tools;
    }

    return payload;
  }

  /**
   * Effectue l'appel API vers Liminality
   */
  private async makeApiCall(payload: LiminalityRequestPayload): Promise<LiminalityResponse> {
    const url = `${this.config.baseUrl}/llm-exec/round`;
    
    // üîç DEBUG: Log de la requ√™te
    logger.dev('[LiminalityProvider] üì§ Requ√™te API:', {
      url,
      model: payload.model,
      hasApiKey: !!this.config.apiKey,
      apiKeyPrefix: this.config.apiKey ? this.config.apiKey.substring(0, 15) + '...' : 'MISSING'
    });
    
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': this.config.apiKey
      },
      body: JSON.stringify(payload),
      signal: AbortSignal.timeout(this.config.timeout)
    });

    if (!response.ok) {
      const errorText = await response.text();
      
      let errorDetails: { error?: string } = {};
      try {
        errorDetails = JSON.parse(errorText);
      } catch {
        // Ignore
      }
      
      const errorMessage = errorDetails.error || errorText;
      throw new Error(`Liminality API error: ${response.status} - ${errorMessage}`);
    }

    return response.json();
  }

  /**
   * Extrait la r√©ponse de l'API Liminality
   */
  private extractResponse(response: LiminalityResponse): {
    content: string;
    tool_calls: ToolCall[];
    usage: LLMResponse['usage'];
    reasoning?: string;
  } {
    const message = response.message;

    // Convertir tool_calls Liminality vers format ToolCall
    const toolCalls: ToolCall[] = [];
    if (message.tool_calls && message.tool_calls.length > 0) {
      for (const tc of message.tool_calls) {
        // Ne traiter que les tool_request (avec id, name, arguments)
        // Les tool_response (avec tool_call_id, content) sont ignor√©s ici
        if (tc.id && tc.name) {
          toolCalls.push({
            id: tc.id,
            type: 'function',
            function: {
              name: tc.name,
              arguments: JSON.stringify(tc.arguments)
            }
          });
        }
      }
    }

    return {
      content: message.content || '',
      tool_calls: toolCalls,
      usage: response.usage,
      reasoning: message.reasoning
    };
  }

  /**
   * Type guard pour valider un LiminalityStreamEvent
   */
  private isValidLiminalityStreamEvent(event: unknown): event is LiminalityStreamEvent {
    if (!event || typeof event !== 'object') {
      return false;
    }
    const e = event as Record<string, unknown>;
    const validTypes = ['start', 'text.delta', 'chunk', 'text.done', 'tool_block.start', 'tool_block.done', 'done', 'tool_call', 'tool_result', 'end', 'error'];
    return typeof e.type === 'string' && validTypes.includes(e.type);
  }

  /**
   * Type guard pour valider un LiminalityToolCallInMessage
   */
  private isValidLiminalityToolCall(tc: unknown): tc is LiminalityToolCallInMessage {
    if (!tc || typeof tc !== 'object') {
      return false;
    }
    const toolCall = tc as Record<string, unknown>;
    const hasValidId = typeof toolCall.id === 'string';
    const hasValidName = typeof toolCall.name === 'string';
    const hasValidArguments = typeof toolCall.arguments === 'string' || 
                              (toolCall.arguments !== null && 
                               toolCall.arguments !== undefined && 
                               typeof toolCall.arguments === 'object');
    return hasValidId && hasValidName && hasValidArguments;
  }

  /**
   * Convertit un event de stream Liminality vers le format StreamChunk
   * 
   * @param event - Event Liminality valid√©
   * @returns StreamChunk ou null si event ignor√©
   * @throws {Error} Si event invalide
   */
  private convertStreamEvent(event: unknown): StreamChunk | null {
    // ‚úÖ VALIDATION STRICTE : Type guard
    if (!this.isValidLiminalityStreamEvent(event)) {
      logger.error('[LiminalityProvider] ‚ùå Event invalide re√ßu', { 
        event: typeof event === 'object' ? JSON.stringify(event) : String(event)
      });
      throw new Error('Invalid Liminality stream event: missing or invalid type');
    }
    switch (event.type) {
      case 'start':
        // Event de d√©marrage, on ne yield rien
        return null;

      case 'text.delta':
        // ‚úÖ Format r√©el de Liminality : text.delta avec "delta"
        return {
          type: 'delta',
          content: event.delta || ''
        };

      case 'chunk':
        // Chunk de contenu (ancien format)
        return {
          type: 'delta',
          content: event.content || ''
        };

      case 'text.done':
        // Fin du texte (on peut ignorer, on a d√©j√† tout via les deltas)
        return null;

      case 'tool_block.start':
        // ‚úÖ D√©but d'un tool call
        logger.dev(`[LiminalityProvider] üîß Tool call start: ${event.block_id}`);
        return null;

      case 'tool_block.done':
        // ‚úÖ Fin d'un tool call (on verra les tool_calls dans le 'done')
        logger.dev(`[LiminalityProvider] ‚úÖ Tool call done: ${event.block_id}`);
        return null;

      case 'done':
        // ‚úÖ Fin du stream avec usage et √©ventuels tool calls
        // Extraire les tool calls du dernier message si pr√©sent
        const lastMessage = event.messages?.[event.messages.length - 1];
        
        if (lastMessage?.role === 'tool_request' && Array.isArray(lastMessage?.tool_calls) && lastMessage.tool_calls.length > 0) {
          logger.info(`[LiminalityProvider] üîß Tool calls d√©tect√©s: ${lastMessage.tool_calls.length}`);
          
          // ‚úÖ VALIDATION STRICTE : Filtrer et valider chaque tool call
          const validToolCalls = lastMessage.tool_calls
            .filter((tc): tc is LiminalityToolCallInMessage => this.isValidLiminalityToolCall(tc));
          
          if (validToolCalls.length !== lastMessage.tool_calls.length) {
            const invalidCount = lastMessage.tool_calls.length - validToolCalls.length;
            logger.warn(`[LiminalityProvider] ‚ö†Ô∏è ${invalidCount} tool call(s) invalide(s) filtr√©(s)`, {
              total: lastMessage.tool_calls.length,
              valid: validToolCalls.length
            });
          }
          
          // ‚úÖ CONVERSION S√âCURIS√âE : Type safety garantie
          const toolCalls: ToolCall[] = validToolCalls.map((tc) => {
            let argumentsString: string;
            try {
              argumentsString = typeof tc.arguments === 'string' 
                ? tc.arguments 
                : JSON.stringify(tc.arguments);
            } catch (stringifyError) {
              logger.error('[LiminalityProvider] ‚ùå Erreur s√©rialisation arguments tool call', {
                toolCallId: tc.id,
                toolName: tc.name,
                error: stringifyError instanceof Error ? stringifyError.message : String(stringifyError)
              });
              // Fallback : arguments vides plut√¥t que crash
              argumentsString = '{}';
            }
            
            return {
              id: tc.id,
              type: 'function' as const,
              function: {
                name: tc.name,
                arguments: argumentsString
              }
            };
          });
          
          // ‚úÖ VALIDATION STRICTE : event.complete peut √™tre undefined
          const isComplete = event.complete === true;
          
          return {
            type: 'delta',
            tool_calls: toolCalls,
            finishReason: isComplete ? 'stop' : 'tool_calls',
            usage: event.usage
          };
        }
        
        return {
          type: 'delta',
          finishReason: 'stop',
          usage: event.usage
        };

      case 'tool_call':
        // Tool call en cours (ancien format)
        logger.dev(`[LiminalityProvider] üîß Tool call: ${event.tool_name}`);
        return {
          type: 'delta',
          content: `[Ex√©cution: ${event.tool_name}]\n`
        };

      case 'tool_result':
        // R√©sultat de tool (ancien format)
        logger.dev(`[LiminalityProvider] ‚úÖ Tool result: ${event.tool_name}`);
        return {
          type: 'delta',
          content: '' // Ne pas afficher le r√©sultat brut
        };

      case 'end':
        // Fin du stream avec usage (ancien format)
        return {
          type: 'delta',
          finishReason: 'stop',
          usage: event.usage
        };

      case 'error':
        // Erreur pendant le stream
        logger.error(`[LiminalityProvider] ‚ùå Stream error:`, event.error);
        throw new Error(event.error?.message || 'Stream error');

      default:
        return null;
    }
  }

  /**
   * R√©cup√®re les capacit√©s du provider
   */
  getCapabilities() {
    return this.info.capabilities;
  }

  /**
   * R√©cup√®re les mod√®les support√©s
   */
  getSupportedModels() {
    return this.info.supportedModels;
  }

  /**
   * R√©cup√®re le pricing
   */
  getPricing() {
    return this.info.pricing;
  }
}

