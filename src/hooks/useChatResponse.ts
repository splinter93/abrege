import { useState, useCallback, useRef } from 'react';
import { simpleLogger as logger } from '@/utils/logger';
import { StreamOrchestrator, type StreamErrorDetails } from '@/services/streaming/StreamOrchestrator';
import { networkRetryService, type NetworkError } from '@/services/network/NetworkRetryService';
import type { ToolCall, ToolResult } from '@/hooks/useChatHandlers';
import type { ChatMessage } from '@/types/chat';
import type { MessageContent } from '@/types/image';
import type { StreamTimeline } from '@/types/streamTimeline';
import type { LLMContextForOrchestrator } from '@/services/chat/ChatContextBuilder';

interface UseChatResponseOptions {
  onComplete?: (
    fullContent: string, 
    fullReasoning: string, 
    toolCalls?: ToolCall[], 
    toolResults?: ToolResult[],
    streamTimeline?: StreamTimeline
  ) => void;
  onError?: (error: string | StreamErrorDetails) => void;
  onToolCalls?: (toolCalls: ToolCall[], toolName: string) => void;
  onToolResult?: (toolName: string, result: unknown, success: boolean, toolCallId?: string) => void;
  onToolExecutionComplete?: (toolResults: ToolResult[]) => void;
  // Callbacks pour streaming
  onStreamChunk?: (content: string) => void;
  onStreamStart?: () => void;
  onStreamEnd?: () => void;
  onToolExecution?: (toolCount: number, toolCalls: ToolCall[]) => void;
  useStreaming?: boolean;
  onAssistantRoundComplete?: (content: string, toolCalls: ToolCall[]) => void;
  // âœ… NOUVEAU : Callback pour info modÃ¨le (debug)
  onModelInfo?: (modelInfo: {
    original: string;
    current: string;
    wasOverridden: boolean;
    reasons: string[];
  }) => void;
}

interface UseChatResponseReturn {
  isProcessing: boolean;
  sendMessage: (
    message: string | MessageContent,
    sessionId: string,
    context?: Record<string, unknown> | LLMContextForOrchestrator,
    history?: ChatMessage[],
    token?: string
  ) => Promise<void>;
  reset: () => void;
}

export function useChatResponse(options: UseChatResponseOptions = {}): UseChatResponseReturn {
  const [isProcessing, setIsProcessing] = useState(false);
  const [pendingToolCalls, setPendingToolCalls] = useState<Set<string>>(new Set());

  // âœ… NOUVEAU: Service pour orchestrer le streaming
  const orchestratorRef = useRef<StreamOrchestrator | null>(null);
  
  // Initialiser l'orchestrator (singleton)
  if (!orchestratorRef.current) {
    orchestratorRef.current = new StreamOrchestrator();
  }

  const { 
    onComplete, 
    onError, 
    onToolCalls, 
    onToolResult, 
    onToolExecutionComplete,
    onStreamChunk,
    onStreamStart,
    onStreamEnd,
    onToolExecution,
    useStreaming = false,
    onAssistantRoundComplete,
  } = options;

  const sendMessage = useCallback(async (message: string | MessageContent, sessionId: string, context?: Record<string, unknown> | LLMContextForOrchestrator, history?: ChatMessage[], token?: string) => {
    try {
      // Extraire un aperÃ§u du message pour le logging
      const messagePreview = typeof message === 'string' 
        ? message.substring(0, 50) + '...'
        : `[Multi-modal: ${message.length} parts]`;
      
      logger.dev('[useChatResponse] ðŸŽ¯ sendMessage appelÃ©:', {
        message: messagePreview,
        isMultiModal: typeof message !== 'string',
        sessionId,
        hasContext: !!context,
        historyLength: history?.length || 0,
        hasToken: !!token,
        useStreaming
      });

      setIsProcessing(true);

      const headers: Record<string, string> = { 'Content-Type': 'application/json' };
      
      // Ajouter le token d'authentification si fourni
      if (token) {
        headers['Authorization'] = `Bearer ${token}`;
      }
      
      // âœ… REFACTORÃ‰ : Mode streaming dÃ©lÃ©guÃ© au StreamOrchestrator
      if (useStreaming) {
        logger.dev('[useChatResponse] ðŸŒŠ Mode streaming activÃ©');

        const skipAddingUserMessage =
          context && typeof context === 'object' && 'skipAddingUserMessage' in context
            ? Boolean((context as { skipAddingUserMessage?: unknown }).skipAddingUserMessage)
            : false;
        
        // âœ… RETRY AUTOMATIQUE : ExÃ©cuter le fetch avec retry pour erreurs rÃ©seau rÃ©cupÃ©rables
        const response = await networkRetryService.executeWithRetry(
          async () => {
            const fetchResponse = await fetch('/api/chat/llm/stream', {
              method: 'POST',
              headers,
              body: JSON.stringify({
                message,
                context: {
                  ...(context || { sessionId }),
                  reasoningOverride: (context && typeof context === 'object' && 'reasoningOverride' in context)
                    ? (context as { reasoningOverride?: 'advanced' | 'general' | 'fast' | null }).reasoningOverride
                    : null
                }, 
                history: history || [],
                sessionId,
                skipAddingUserMessage
              })
            });

            if (!fetchResponse.ok) {
              // CrÃ©er une NetworkError typÃ©e pour le retry service
              const error = networkRetryService.createNetworkError(fetchResponse);
              throw error;
            }

            return fetchResponse;
          },
          {
            maxRetries: 3,
            initialDelay: 1000,
            backoffMultiplier: 2,
            maxDelay: 10000,
            operationName: 'stream-llm-request'
          }
        );

        // âœ… DÃ©lÃ©guer au StreamOrchestrator
        const orchestrator = orchestratorRef.current!;
        orchestrator.reset(); // Reset pour nouveau stream
        
        await orchestrator.processStream(response, {
          onStreamStart,
          onStreamChunk,
          onStreamEnd,
          onToolCalls,
          onToolExecution,
          onToolResult,
          onComplete,
          onError,
          onModelInfo: options.onModelInfo // âœ… NOUVEAU : Passer callback modelInfo
        });

        return;
      }
      
      // âœ… MODE CLASSIQUE (sans streaming)
      logger.dev('[useChatResponse] ðŸš€ Envoi de la requÃªte Ã  l\'API LLM (mode classique)');
      
      // âœ… RETRY AUTOMATIQUE : ExÃ©cuter le fetch avec retry pour erreurs rÃ©seau rÃ©cupÃ©rables
      const response = await networkRetryService.executeWithRetry(
        async () => {
          const fetchResponse = await fetch('/api/chat/llm', {
            method: 'POST',
            headers,
            body: JSON.stringify({
              message,
              context: context || { sessionId }, 
              history: history || [],
              sessionId
            })
          });

          if (!fetchResponse.ok) {
            // CrÃ©er une NetworkError typÃ©e pour le retry service
            const error = networkRetryService.createNetworkError(fetchResponse);
            throw error;
          }

          return fetchResponse;
        },
        {
          maxRetries: 3,
          initialDelay: 1000,
          backoffMultiplier: 2,
          maxDelay: 10000,
          operationName: 'llm-request'
        }
      );

      logger.dev('[useChatResponse] âœ… Fetch terminÃ©, traitement de la rÃ©ponse...');

      logger.dev('[useChatResponse] ðŸ“¥ RÃ©ponse HTTP reÃ§ue:', {
        status: response.status,
        ok: response.ok,
        statusText: response.statusText,
        headers: Object.fromEntries(response.headers.entries())
      });

      if (!response.ok) {
        const errorText = await response.text();
        let errorData;
        try {
          errorData = JSON.parse(errorText);
        } catch {
          errorData = { error: `HTTP ${response.status}: ${response.statusText}` };
        }
        
        logger.dev('[useChatResponse] âŒ RÃ©ponse HTTP non-OK:', {
          status: response.status,
          statusText: response.statusText,
          errorText: errorText.substring(0, 500),
          errorData: errorData
        });
        
        throw new Error(errorData.error || `HTTP ${response.status}: ${response.statusText}`);
      }

      logger.dev('[useChatResponse] ðŸ”„ DÃ©but du parsing JSON...');
      
      let data;
      try {
        data = await response.json();
        logger.dev('[useChatResponse] âœ… JSON parsÃ© avec succÃ¨s');
        logger.dev('[useChatResponse] ðŸ” RÃ©ponse brute reÃ§ue:', {
          status: response.status,
          ok: response.ok,
          data: data,
          dataType: typeof data,
          hasSuccess: 'success' in data,
          success: data?.success,
          hasContent: 'content' in data,
          content: data?.content?.substring(0, 100) + '...',
          contentLength: data?.content?.length || 0
        });
      } catch (parseError) {
        logger.dev('[useChatResponse] âŒ Erreur parsing JSON:', {
          error: parseError instanceof Error ? parseError.message : String(parseError)
        });
        const textResponse = await response.text();
        logger.dev('[useChatResponse] âŒ RÃ©ponse texte brute:', {
          response: textResponse.substring(0, 500)
        });
        throw new Error('Erreur parsing JSON de la rÃ©ponse');
      }

      // ðŸ”§ AMÃ‰LIORATION: Validation de base de la rÃ©ponse
      if (!data) {
        throw new Error('RÃ©ponse vide du serveur');
      }

      // âœ… NOUVELLE LOGIQUE CLAIRE ET ROBUSTE
      if (data.success) {
        logger.dev('[useChatResponse] ðŸ” Structure de la rÃ©ponse:', {
          success: data.success,
          is_relance: data.is_relance,
          tool_calls_length: data.tool_calls?.length || 0,
          tool_results_length: data.tool_results?.length || 0,
          content_length: data.content?.length || 0
        });

        // ðŸŽ¯ PRIORITÃ‰ 1 : RÃ©ponse finale (is_relance = true)
        // Le LLM a terminÃ© son traitement aprÃ¨s avoir utilisÃ© des tools
        if (data.is_relance) {
          logger.dev('[useChatResponse] âœ… RÃ©ponse finale aprÃ¨s tool calls reÃ§ue');
          logger.tool('[useChatResponse] âœ… RÃ©ponse finale reÃ§ue');
          
          // Traiter les tool results si prÃ©sents
          if (data.tool_results && data.tool_results.length > 0) {
            for (const toolResult of data.tool_results) {
              onToolResult?.(
                toolResult.name,
                toolResult.result,
                toolResult.success,
                toolResult.tool_call_id
              );
            }
          }
          
          // Informer si des tools ont Ã©chouÃ©
          if (data.has_failed_tools) {
            logger.dev('[useChatResponse] âš ï¸ Des tools ont Ã©chouÃ© mais le LLM a gÃ©rÃ© intelligemment');
          }
          
          // âœ… Toujours appeler onComplete pour une rÃ©ponse finale
          logger.dev('[useChatResponse] ðŸŽ¯ Appel onComplete (rÃ©ponse finale)');
          
          onComplete?.(
            data.content || '', 
            data.reasoning || '', 
            data.tool_calls || [], 
            data.tool_results || [],
            undefined // Pas de timeline en mode classique (non-streaming)
          );
          return;
        }
        
        // ðŸŽ¯ PRIORITÃ‰ 2 : Tool calls en cours (is_relance = false)
        if (data.tool_calls && data.tool_calls.length > 0) {
          const toolCallIds = data.tool_calls.map((tc: { id: string }) => tc.id);
          setPendingToolCalls(new Set(toolCallIds));
          
          if (data.tool_calls.length > 10) {
            logger.tool(`[useChatResponse] âš¡ ${data.tool_calls.length} tool calls dÃ©tectÃ©s`);
          } else {
            logger.tool(`[useChatResponse] ðŸ”§ ${data.tool_calls.length} tool call(s) dÃ©tectÃ©(s)`);
          }
          
          onToolCalls?.(data.tool_calls, 'tool_chain');
          
          // Traiter les tool results s'ils sont dÃ©jÃ  disponibles
          if (data.tool_results && data.tool_results.length > 0) {
            for (const toolResult of data.tool_results) {
              onToolResult?.(
                toolResult.name,
                toolResult.result,
                toolResult.success,
                toolResult.tool_call_id
              );
              
              // Marquer ce tool call comme terminÃ©
              setPendingToolCalls(prev => {
                const newSet = new Set(prev);
                newSet.delete(toolResult.tool_call_id);
                return newSet;
              });
            }

            // Logger la progression
            const completedCount = data.tool_results.length;
            const totalCount = data.tool_calls.length;
            
            if (completedCount === totalCount) {
              logger.dev(`[useChatResponse] âœ… Tous les ${totalCount} tool calls terminÃ©s`);
              onToolExecutionComplete?.(data.tool_results);
            } else {
              logger.dev(`[useChatResponse] â³ ${completedCount}/${totalCount} tool calls terminÃ©s`);
            }
          }
          
          // âŒ NE PAS appeler onComplete ici - attendre la rÃ©ponse finale
          return;
        }
        
        // ðŸŽ¯ PRIORITÃ‰ 3 : RÃ©ponse simple sans tool calls
        logger.dev('[useChatResponse] âœ… RÃ©ponse simple sans tool calls');
        logger.dev('[useChatResponse] ðŸŽ¯ Appel onComplete (rÃ©ponse simple)');
        
        onComplete?.(
          data.content || '', 
          data.reasoning || '', 
          [], 
          []
        );
        return;
      } else {
        // ðŸ”§ AMÃ‰LIORATION: Gestion des erreurs serveur
        if (data.error) {
          const errorMessage = data.error || data.details || 'Erreur inconnue du serveur';
          logger.warn('[useChatResponse] âš ï¸ RÃ©ponse d\'erreur du serveur:', { error: errorMessage, data });
          throw new Error(errorMessage);
        } else {
          // Fallback pour les rÃ©ponses invalides
          logger.warn('[useChatResponse] âš ï¸ RÃ©ponse invalide du serveur:', data);
          throw new Error('Format de rÃ©ponse invalide');
        }
      }
    } catch (error) {
      // âœ… Gestion d'erreur avec dÃ©tection NetworkError
      let errorMessage: string;
      let errorDetails: string | StreamErrorDetails | undefined;
      
      if (error instanceof Error) {
        const networkError = error as NetworkError;
        
        // Si c'est une NetworkError aprÃ¨s retry, enrichir le message
        if (networkError.isRecoverable !== undefined) {
          errorMessage = networkError.isRecoverable
            ? `Erreur rÃ©seau aprÃ¨s plusieurs tentatives : ${networkError.message}`
            : networkError.message;
          
          // CrÃ©er StreamErrorDetails si c'est une erreur de streaming
          if (networkError.statusCode) {
            errorDetails = {
              error: errorMessage,
              statusCode: networkError.statusCode,
              errorCode: networkError.errorType, // StreamErrorDetails utilise errorCode, pas errorType
              timestamp: Date.now(),
              recoverable: networkError.isRecoverable
            };
          }
        } else {
          errorMessage = error.message;
        }
        
        logger.error('[useChatResponse] âŒ Erreur:', {
          message: errorMessage,
          stack: error instanceof Error ? error.stack : undefined,
          isNetworkError: networkError.isRecoverable !== undefined,
          recoverable: networkError.isRecoverable
        });
      } else if (typeof error === 'string') {
        errorMessage = error;
        logger.error('[useChatResponse] âŒ Erreur:', { error });
      } else {
        errorMessage = 'Erreur inconnue';
        logger.error('[useChatResponse] âŒ Erreur:', { error: String(error) });
      }
      
      // Appeler le callback d'erreur si disponible (avec dÃ©tails si disponibles)
      if (errorDetails) {
        onError?.(errorDetails);
      } else {
        onError?.(errorMessage);
      }
    } finally {
      setIsProcessing(false);
    }
  }, [onComplete, onError, onToolCalls, onToolResult, onToolExecutionComplete, onStreamChunk, onStreamStart, onStreamEnd, onToolExecution, useStreaming, onAssistantRoundComplete]);

  const reset = useCallback(() => {
    setIsProcessing(false);
    setPendingToolCalls(new Set());
  }, []);

  return {
    isProcessing,
    sendMessage,
    reset
  };
} 